{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuhbCKQl3iQE"
      },
      "source": [
        "#Instalacion de las librerías necesarias:\n",
        "\n",
        "gradio: UI web para el chat.\n",
        "\n",
        "groq: API cliente del modelo LLM de Groq.\n",
        "\n",
        "pinecone-client: para vectores e índices.\n",
        "\n",
        "sentence-transformers: para generar embeddings.\n",
        "\n",
        "langchain-text-splitters: para dividir texto.\n",
        "\n",
        "pypdf: para leer PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IXCRYSFXvxM",
        "outputId": "5d82922a-4e9e-43ca-fd0f-96878198d406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install gradio groq pinecone-client sentence-transformers langchain-text-splitters pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9upZEWaAeU22",
        "outputId": "375647ec-e406-40ca-a7ac-feb38f494180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "3wFutQTCei6u",
        "outputId": "61f2e6a7-5d31-47d9-a0a0-7334be1d6cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.8.3)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.14.1)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Collecting packaging<25.0,>=24.2 (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging, pinecone-plugin-assistant, pinecone\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "Successfully installed packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.7.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "6836147c71454643a5375fb782cd0bdc",
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2ddgUQR97-eE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import gradio as gr\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "from pinecone import (\n",
        "    Pinecone,\n",
        "    ServerlessSpec,\n",
        "    CloudProvider,\n",
        "    AwsRegion,\n",
        "    VectorType\n",
        ")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from pypdf import PdfReader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ybr3gHG_0BW"
      },
      "source": [
        "# Configuruacion y testeos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoGlaxMS4VSS"
      },
      "source": [
        "Obtiene las API keys desde los \"secrets\" de Colab. Si no existen, lanza un error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXkcNdcN4Rir",
        "outputId": "0bb1122b-904e-4e5d-d8d9-c0bcc8094f2a"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Secrets (Colab) - con fallback a input\n",
        "# =========================\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "def get_secret_or_input(key_name: str, prompt: str) -> str:\n",
        "    \"\"\"Intenta obtener un secreto de Colab.\n",
        "    Si no existe, lo pide por consola.\"\"\"\n",
        "    try:\n",
        "        value = userdata.get(key_name)\n",
        "        if value:\n",
        "            return value.strip()\n",
        "    except Exception:\n",
        "        # SecretNotFoundError u otro\n",
        "        pass\n",
        "    # Si no está en userdata, lo pide manualmente\n",
        "    return input(f\"👉 Ingresá tu {prompt}: \").strip()\n",
        "\n",
        "GROQ_API_KEY = get_secret_or_input(\"GROQ_API_KEY\", \"GROQ_API_KEY\")\n",
        "PINECONE_API_KEY = get_secret_or_input(\"PINECONE_API_KEY\", \"PINECONE_API_KEY\")\n",
        "\n",
        "# Guardar en variables de entorno\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "print(\"✅ Claves configuradas correctamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-yzFSZBAO3Y",
        "outputId": "97c31b59-a20e-4e42-dd1c-1f8f4dbb5408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ GROQ OK: OK GROQ\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Test GROQ\n",
        "# =========================\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "assert os.environ.get(\"GROQ_API_KEY\"), \"❌ Falta GROQ_API_KEY en os.environ\"\n",
        "\n",
        "try:\n",
        "    client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "    chat = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Sos un asistente muy conciso.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Decime 'OK GROQ' si me leés bien.\"}\n",
        "        ],\n",
        "        max_tokens=8,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    print(\"✅ GROQ OK:\", chat.choices[0].message.content.strip())\n",
        "except Exception as e:\n",
        "    print(\"❌ Error GROQ:\", type(e).__name__, str(e))\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r1lDc9HEARZW"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import os, time, uuid, numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AIaEzO-ko8wj"
      },
      "outputs": [],
      "source": [
        "def pinecone_reset_all(api_key=None, *, realmente_borrar=False, wait=True, timeout=120, poll_every=2.0):\n",
        "    \"\"\"\n",
        "    Borra TODOS los índices de Pinecone (irreversible).\n",
        "    - Si `realmente_borrar=False` (por defecto), solo muestra qué borraría (dry-run).\n",
        "    - Si `wait=True`, espera a que desaparezcan del listado (hasta `timeout` segundos).\n",
        "\n",
        "    Uso:\n",
        "      # ver qué hay (dry-run)\n",
        "      pinecone_reset_all()\n",
        "\n",
        "      # borrar de verdad\n",
        "      pinecone_reset_all(realmente_borrar=True)\n",
        "    \"\"\"\n",
        "    import os, time\n",
        "    from pinecone import Pinecone\n",
        "\n",
        "    # Obtener API key (env o Colab)\n",
        "    api_key = api_key or os.getenv(\"PINECONE_API_KEY\")\n",
        "    if not api_key:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Falta PINECONE_API_KEY (en entorno o en Colab > Secrets).\")\n",
        "\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "\n",
        "    items = pc.list_indexes()\n",
        "\n",
        "    # Compatibilidad con distintas formas de listado\n",
        "    def _name(x):\n",
        "        if isinstance(x, str): return x\n",
        "        n = getattr(x, \"name\", None)\n",
        "        if isinstance(n, str): return n\n",
        "        if isinstance(x, dict): return x.get(\"name\")\n",
        "        return None\n",
        "\n",
        "    names = [n for n in map(_name, items) if n]\n",
        "\n",
        "    if not realmente_borrar:\n",
        "        return {\"accion\": \"dry-run\", \"encontrados\": names}\n",
        "\n",
        "    for n in names:\n",
        "        pc.delete_index(n)\n",
        "\n",
        "    if not wait:\n",
        "        return {\"accion\": \"deleted\", \"borrados\": names}\n",
        "\n",
        "    # Polling hasta que no aparezcan\n",
        "    t0 = time.time()\n",
        "    remaining = names[:]\n",
        "    while time.time() - t0 < timeout:\n",
        "        now = [n for n in map(_name, pc.list_indexes()) if n]\n",
        "        remaining = [n for n in remaining if n in now]\n",
        "        if not remaining:\n",
        "            break\n",
        "        time.sleep(poll_every)\n",
        "\n",
        "    return {\"accion\": \"deleted\", \"borrados\": names, \"restantes\": remaining}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RR1mqGkpC_E",
        "outputId": "785b1166-91a1-4385-bd66-08a1fb2e7cd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accion': 'dry-run', 'encontrados': ['cv--cv1', 'cv--cv2']}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1) Ver qué índices hay (no borra)\n",
        "pinecone_reset_all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRdBQdeSpJJB",
        "outputId": "8a8b64b4-df7e-4812-80c6-4601c2c06225"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accion': 'deleted', 'borrados': ['cv--cv1', 'cv--cv2'], 'restantes': []}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2) Borrar todo y esperar confirmación\n",
        "pinecone_reset_all(realmente_borrar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFVzSWP3pL0B",
        "outputId": "24c94554-5834-48c8-dc4e-151cd1876537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accion': 'dry-run', 'encontrados': []}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1) Ver qué índices hay (no borra)\n",
        "pinecone_reset_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gus3ypi37yi"
      },
      "source": [
        "#Definicion:\n",
        "\n",
        "Directorio de documentos (/content/cv)\n",
        "\n",
        "Modelo de embeddings (MiniLM)\n",
        "\n",
        "Tamaño y solapamiento de chunks\n",
        "\n",
        "Región y proveedor para Pinecone\n",
        "\n",
        "Parámetros default para generación (top_p, temperature, max_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p90mH4m6ErM6"
      },
      "outputs": [],
      "source": [
        "# Carpeta destino\n",
        "!mkdir -p /content/cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsKBuQtoAtdo",
        "outputId": "f9452fba-4b0f-465e-f0ab-0496f01ed3c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-08-24 13:50:34--  https://raw.githubusercontent.com/ctalamilla/UBA-NLP2-TP3/refs/heads/main/cv2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1389 (1.4K) [text/plain]\n",
            "Saving to: ‘/content/cv/cv2.txt’\n",
            "\n",
            "\rcv2.txt               0%[                    ]       0  --.-KB/s               \rcv2.txt             100%[===================>]   1.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-24 13:50:34 (17.3 MB/s) - ‘/content/cv/cv2.txt’ saved [1389/1389]\n",
            "\n",
            "--2025-08-24 13:50:34--  https://raw.githubusercontent.com/ctalamilla/UBA-NLP2-TP3/refs/heads/main/cv1.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1405 (1.4K) [text/plain]\n",
            "Saving to: ‘/content/cv/cv1.txt’\n",
            "\n",
            "cv1.txt             100%[===================>]   1.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-24 13:50:35 (20.0 MB/s) - ‘/content/cv/cv1.txt’ saved [1405/1405]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Lista de URLs (una por línea)\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/ctalamilla/UBA-NLP2-TP3/refs/heads/main/cv2.txt\",\n",
        "    \"https://raw.githubusercontent.com/ctalamilla/UBA-NLP2-TP3/refs/heads/main/cv1.txt\"\n",
        "    ]\n",
        "\n",
        "# Descargar todos en la carpeta /content/cv\n",
        "for u in urls:\n",
        "    !wget -nc -P /content/cv \"{u}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Q-qxS4mo7-26"
      },
      "outputs": [],
      "source": [
        "DOCS_DIR = \"/content/cv\"                   # (.pdf / .txt)\n",
        "INDEX_PREFIX = \"cv--\"                       # Prefijo para índices por CV\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "CHUNK_SIZE = 200\n",
        "CHUNK_OVERLAP = 25\n",
        "EMB_DIM = 384                              # all-MiniLM-L6-v2 -> 384\n",
        "PINECONE_CLOUD = CloudProvider.AWS\n",
        "PINECONE_REGION = AwsRegion.US_EAST_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-t6TQ3Vg7-6C"
      },
      "outputs": [],
      "source": [
        "# Parámetros de generación por defecto (Se ajustan desde la interfase)\n",
        "TOP_K_DEFAULT = 4\n",
        "TEMPERATURE_DEFAULT = 0.7\n",
        "TOP_P_DEFAULT = 0.9\n",
        "MAX_TOKENS_DEFAULT = 512\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Eres un asistente que responde usando EXCLUSIVAMENTE el contexto proporcionado. \"\n",
        "    \"Si el contexto no contiene la respuesta, dilo con claridad. Sé breve, claro y útil.\"\n",
        ")\n",
        "\n",
        "ROUTER_SYSTEM = (\n",
        "    \"Eres un enrutador. Tu tarea es elegir UN ÚNICO índice de la lista que mejor \"\n",
        "    \"corresponda a la consulta del usuario. Responde SOLO con el nombre EXACTO del índice y nada más. \"\n",
        "    \"Si ninguno aplica, responde 'NONE'.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "JZfUCnu6HNDz"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Groq client\n",
        "# =========================\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# =========================\n",
        "# Pinecone client\n",
        "# =========================\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "s8HHVuNY4kpX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUND2oUW4sLN"
      },
      "source": [
        "#Embeddings y Chunking\n",
        "\n",
        "Divide texto largo en chunks superpuestos.\n",
        "\n",
        "Calcula los embeddings de cada chunk (o uno solo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ZWIj6TJmHTLu"
      },
      "outputs": [],
      "source": [
        "# Embeddings\n",
        "emb_model = SentenceTransformer(EMB_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5VVlRGde40PQ"
      },
      "outputs": [],
      "source": [
        "# Chunking\n",
        "def chunk_text(text: str) -> List[str]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        separators=[\" \", \"\"]\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    return [vec.tolist() for vec in emb_model.encode(texts)]\n",
        "\n",
        "def embed_text(text: str) -> List[float]:\n",
        "    return emb_model.encode([text])[0].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiGBWUfa48si"
      },
      "source": [
        "#Limpieza del texto\n",
        "\n",
        "Elimina tildes y diéresis (usando unicodedata.normalize).\n",
        "\n",
        "Convierte la ñ a n.\n",
        "\n",
        "Elimina caracteres no ASCII y nulos.\n",
        "\n",
        "Reduce espacios múltiples a uno solo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "99hyKHTw5IOZ"
      },
      "outputs": [],
      "source": [
        "# Limpieza del texto\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    # Reemplaza caracteres acentuados por sus equivalentes no acentuados\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    text = \"\".join([c for c in text if not unicodedata.combining(c)])\n",
        "    # Reemplazar ñ y Ñ por n y N\n",
        "    text = text.replace(\"ñ\", \"n\").replace(\"Ñ\", \"N\")\n",
        "    # Eliminar caracteres no ASCII\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    # Eliminar nulos\n",
        "    text = text.replace(\"\\x00\", \"\")\n",
        "    # Colapsar múltiples espacios\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_PpuUD66-_k"
      },
      "source": [
        "#Lectura de archivos\n",
        "\n",
        "Lee archivos .txt como latin-1.\n",
        "\n",
        "Lee PDFs usando PdfReader.\n",
        "\n",
        "Usa clean_text() en ambos casos.\n",
        "\n",
        "read_file() elige el método según la extensión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "gyXfl8Kfm33J"
      },
      "outputs": [],
      "source": [
        "# Lectura de archivos\n",
        "\n",
        "def read_txt(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
        "        raw = f.read()\n",
        "        return clean_text(raw)\n",
        "\n",
        "def read_pdf(path: str) -> str:\n",
        "    text_parts = []\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        for page in reader.pages:\n",
        "            text_parts.append(page.extract_text() or \"\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return clean_text(\"\\n\".join(text_parts))\n",
        "\n",
        "def read_file(path: Path) -> str:\n",
        "    ext = path.suffix.lower()\n",
        "    if ext == \".txt\":\n",
        "        return read_txt(str(path))\n",
        "    elif ext == \".pdf\":\n",
        "        return read_pdf(str(path))\n",
        "    return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "KYB_teFuJl4h",
        "outputId": "5f82e127-1035-4ab2-f2ec-d0ccfe95c3bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# CV Corporativo - Ricardo Lopez ## Datos Personales - Nombre: Ricardo Lopez - Edad: 47 anios - Correo: ricardo.lopez.corporativo@example.com - Ubicacion: Buenos Aires, Argentina - LinkedIn: linkedin.com/in/ricardolopez-corporativo ## Resumen Profesional - Ejecutivo con experiencia en liderazgo empresarial y gestion de proyectos tecnologicos. - Especialista en transformacion digital y gestion de equipos multidisciplinarios. - Enfoque en cumplimiento de KPIs y desarrollo de cultura basada en datos. ## Experiencia Laboral - **Director de Transformacion Digital en TechCorp**: implementacion de estrategias de innovacion. - **Gerente de Proyectos en Empresa XYZ**: liderazgo de equipos en telecomunicaciones y finanzas. - **Consultor Senior en Deloitte**: asesoria en analitica de datos y gestion de riesgos. ## Logros Destacados - Reduccion de costos operativos en un 20% mediante procesos de digitalizacion. - Implementacion de tableros de control para seguimiento de indicadores clave. - Desarrollo de programas de capacitacion en gestion agil y data-driven. ## Educacion - MBA - Universidad Austral. - Maestria en Ciencia de Datos - UBA. - Ingenieria en Sistemas - Universidad Nacional. ## Habilidades - Gestion de proyectos, liderazgo, transformacion digital. - Power BI, Tableau, SQL, Python. - Negociacion con stakeholders y gestion de presupuestos.'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_text(read_txt('/content/cv/cv1.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULcnMWx579XP"
      },
      "source": [
        "#Gestión de índices de Pinecone\n",
        "\n",
        "Convierte nombres de archivo en nombres válidos para índices Pinecone.\n",
        "\n",
        "Lista los índices existentes.\n",
        "\n",
        "Crea un índice si no existe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "uyVcEybc_d7l"
      },
      "outputs": [],
      "source": [
        "# Helpers de índices por CV\n",
        "\n",
        "def sanitize_index_name(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Pinecone index name: lowercase, alfanumérico y guiones, 1-45 chars, comenzar con letra.\n",
        "    Creamos: f\"cv_{slug}\" donde slug se arma desde el nombre del archivo (sin extensión).\n",
        "    \"\"\"\n",
        "    slug = name.lower()\n",
        "    slug = re.sub(r\"\\s+\", \"-\", slug)\n",
        "    slug = re.sub(r\"[^a-z0-9-]\", \"-\", slug)\n",
        "    slug = re.sub(r\"-{2,}\", \"-\", slug).strip(\"-\")\n",
        "    if not slug or not slug[0].isalpha():\n",
        "        slug = \"a\" + slug\n",
        "    slug = slug[:40]  # dejamos margen para \"cv_\"\n",
        "    return f\"{INDEX_PREFIX}{slug}\"\n",
        "\n",
        "def list_cv_indices() -> List[str]:\n",
        "    listing = pc.list_indexes()\n",
        "    names = [item[\"name\"] for item in listing] if isinstance(listing, list) else [d.get(\"name\") for d in listing]\n",
        "    return sorted([n for n in names if n and n.startswith(INDEX_PREFIX)])\n",
        "\n",
        "def ensure_index(index_name: str):\n",
        "    existing = list_cv_indices()\n",
        "    if index_name not in existing:\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=EMB_DIM,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(\n",
        "                cloud=PINECONE_CLOUD,\n",
        "                region=PINECONE_REGION\n",
        "            ),\n",
        "            vector_type=VectorType.DENSE\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVQYBH1Z_Vk5"
      },
      "source": [
        "#Carga de documentos a Pinecone\n",
        "\n",
        "Para cada .txt o .pdf:\n",
        "\n",
        "Limpia y divide el texto.\n",
        "\n",
        "Calcula embeddings.\n",
        "\n",
        "Crea vectores con metadatos.\n",
        "\n",
        "Hace upsert (insertar o actualizar) en el índice correspondiente (uno por archivo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "uz5zaXB8_hwG"
      },
      "outputs": [],
      "source": [
        "# Ingesta/Upsert: UN índice por CV\n",
        "\n",
        "def upsert_documents_per_cv(folder: str) -> str:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    paths = [Path(p) for p in glob.glob(os.path.join(folder, \"*.pdf\"))] + \\\n",
        "            [Path(p) for p in glob.glob(os.path.join(folder, \"*.txt\"))]\n",
        "    if not paths:\n",
        "        return \"No se encontraron documentos (.pdf/.txt) en la carpeta.\"\n",
        "\n",
        "    total_chunks = 0\n",
        "    created = 0\n",
        "    updated = 0\n",
        "\n",
        "    for path in paths:\n",
        "        base_noext = path.stem\n",
        "        index_name = sanitize_index_name(base_noext)\n",
        "\n",
        "        # Asegurar índice para este CV\n",
        "        before = set(list_cv_indices())\n",
        "        ensure_index(index_name)\n",
        "        after = set(list_cv_indices())\n",
        "        if index_name in (after - before):\n",
        "            created += 1\n",
        "        else:\n",
        "            updated += 1\n",
        "\n",
        "        # Leer y trocear\n",
        "        raw_text = read_file(path).strip()\n",
        "        if not raw_text:\n",
        "            print(f\"Vacío o ilegible: {path.name}\")\n",
        "            continue\n",
        "\n",
        "        chunks = chunk_text(raw_text)\n",
        "        if not chunks:\n",
        "            print(f\"Sin chunks: {path.name}\")\n",
        "            continue\n",
        "\n",
        "        vecs = embed_texts(chunks)\n",
        "        vectors = []\n",
        "        for i, vec in enumerate(vecs):\n",
        "            vid = f\"{base_noext}-{i:04d}\"\n",
        "            vectors.append({\n",
        "                \"id\": vid,\n",
        "                \"values\": vec,\n",
        "                \"metadata\": {\n",
        "                    \"texto\": chunks[i],\n",
        "                    \"archivo\": path.name,\n",
        "                    \"chunk_id\": i,\n",
        "                    \"fecha\": datetime.today().strftime(\"%Y-%m-%d\")\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Upsert al índice del CV\n",
        "        idx = pc.Index(index_name)\n",
        "        # Upsert por lotes\n",
        "        BATCH = 200\n",
        "        for i in range(0, len(vectors), BATCH):\n",
        "            idx.upsert(vectors=vectors[i:i+BATCH])\n",
        "\n",
        "        total_chunks += len(chunks)\n",
        "\n",
        "    return f\"Indexado OK. Archivos: {len(paths)} | Índices nuevos: {created} | Actualizados: {updated} | Chunks insertados: {total_chunks}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwOLiIK5_mJd"
      },
      "source": [
        "#Consulta por vector (Retrieve)\n",
        "\n",
        "Usa el vector de la query para buscar en un índice de Pinecone.\n",
        "\n",
        "Devuelve los top_k matches.\n",
        "\n",
        "Arma texto con los resultados y sus fuentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Nei6ThGE_rvG"
      },
      "outputs": [],
      "source": [
        "# Retrieve (consulta vectorial) desde un índice concreto\n",
        "\n",
        "def retrieve_from_index(index_name: str, query: str, top_k: int = 4) -> List[Dict[str, Any]]:\n",
        "    idx = pc.Index(index_name)\n",
        "    qvec = embed_text(query)\n",
        "    res = idx.query(vector=qvec, top_k=int(top_k), include_metadata=True)\n",
        "\n",
        "    matches = res.get(\"matches\", []) if isinstance(res, dict) else getattr(res, \"matches\", [])\n",
        "    out = []\n",
        "    for m in matches or []:\n",
        "        score = m.get(\"score\") if isinstance(m, dict) else getattr(m, \"score\", None)\n",
        "        meta = m.get(\"metadata\", {}) if isinstance(m, dict) else getattr(m, \"metadata\", {}) or {}\n",
        "        out.append({\n",
        "            \"score\": round(float(score or 0.0), 4),\n",
        "            \"texto\": meta.get(\"texto\", \"\"),\n",
        "            \"archivo\": meta.get(\"archivo\", \"\")\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def build_context(matches: List[Dict[str, Any]]) -> str:\n",
        "    if not matches:\n",
        "        return \"(no se recuperaron pasajes relevantes)\"\n",
        "    parts = []\n",
        "    for i, m in enumerate(matches, 1):\n",
        "        parts.append(f\"[{i}] ({m['archivo']}, score={m['score']})\\n{m['texto']}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIWw4Kk9_vpe"
      },
      "source": [
        "#Router multiagente\n",
        "\n",
        "Usa un LLM (Groq) para decidir cuál índice (archivo) es más relevante para una pregunta.\n",
        "\n",
        "Solo devuelve el nombre del índice (o NONE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "YdU41Zbqm_hy"
      },
      "outputs": [],
      "source": [
        "# Router Agent: elegir ÍNDICE del CV\n",
        "\n",
        "def router_agent(user_question: str, candidates: List[str]) -> Optional[str]:\n",
        "    if not candidates:\n",
        "        return None\n",
        "    cand_list = \"\\n\".join(f\"- {c}\" for c in candidates)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": ROUTER_SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": f\"Índices disponibles:\\n{cand_list}\\n\\nPregunta: {user_question}\\n\\nDevuelve SOLO un nombre EXACTO de la lista, o 'NONE' si ninguno aplica.\"}\n",
        "    ]\n",
        "    try:\n",
        "        r = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=messages,\n",
        "            temperature=0.0,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        choice = (r.choices[0].message.content or \"\").strip().splitlines()[0].strip()\n",
        "        if choice.upper() == \"NONE\":\n",
        "            return None\n",
        "        return choice if choice in candidates else None\n",
        "    except Exception:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPF7cMTTAm4E"
      },
      "source": [
        "#Verificar chunks\n",
        "\n",
        "Muestra el primer chunk de cada archivo.\n",
        "\n",
        "Sirve como herramienta de debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "04TJfz_sQRkN"
      },
      "outputs": [],
      "source": [
        "# Verificación de chunks (primer chunk por CV)\n",
        "def verificar_chunks():\n",
        "    import os\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "    textos_por_cv = []\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "    for fname in os.listdir(DOCS_DIR):\n",
        "        filepath = os.path.join(DOCS_DIR, fname)\n",
        "        if not os.path.isfile(filepath):\n",
        "            continue\n",
        "        if not (fname.endswith(\".txt\") or fname.endswith(\".pdf\")):\n",
        "            continue\n",
        "\n",
        "        # Leer archivo con robustez\n",
        "        try:\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(filepath, \"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "        # Limpiar texto\n",
        "        text = text.replace(\"\\x00\", \"\").strip()\n",
        "\n",
        "        # Dividir en chunks\n",
        "        chunks = splitter.split_text(text)\n",
        "        primer_chunk = chunks[0] if chunks else \"(vacío o ilegible)\"\n",
        "        textos_por_cv.append(f\"**{fname}**\\n{primer_chunk}\")\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(textos_por_cv) or \" No se encontraron archivos válidos.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "7hPfk6e_Kcd5",
        "outputId": "0e69e92b-94d0-43ec-e42a-f3ca39657b71"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**cv1.txt**\\n# CV Corporativo - Ricardo Lopez\\n\\n## Datos Personales\\n- Nombre: Ricardo Lopez  \\n- Edad: 47 anios  \\n- Correo: ricardo.lopez.corporativo@example.com  \\n- Ubicacion: Buenos Aires, Argentina  \\n- LinkedIn: linkedin.com/in/ricardolopez-corporativo\\n\\n---\\n\\n**cv2.txt**\\n# CV Tecnologico - Sofia Martinez\\n\\n## Datos Personales\\n- Nombre: Sofia Martinez  \\n- Edad: 38 anios  \\n- Correo: sofia.martinez.tecnologico@example.com  \\n- Ubicacion: Mendoza, Argentina  \\n- LinkedIn: linkedin.com/in/sofiamartinez-tecnologico  \\n\\n## Resumen Profesional\\n- Especialista en desarrollo de software, arquitecturas de datos e inteligencia artificial.  \\n- Experiencia en startups y empresas multinacionales.  \\n- Enfoque en soluciones escalables, seguras y eficientes en la nube.'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "verificar_chunks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xywxEijABqF"
      },
      "source": [
        "#Chat multi-agente (RAG)\n",
        "\n",
        "Usa el router para elegir índice.\n",
        "\n",
        "Recupera contexto con vectores.\n",
        "\n",
        "Llama al modelo LLM con contexto, pregunta e historial.\n",
        "\n",
        "Devuelve respuesta + fuentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OXaCLf6Xjbt",
        "outputId": "4edd157c-6859-4dc3-d453-3892274f298b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_cv_indices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "s0MYWw25AUV9"
      },
      "outputs": [],
      "source": [
        "def chat(user_message, chat_history):\n",
        "    \"\"\"\n",
        "    - Router elige el índice (CV) más adecuado.\n",
        "    - Retrieve desde ese índice (usando PARAMS['k']).\n",
        "    - Groq responde con el contexto recuperado (usando temp/top_p/max_tokens de PARAMS).\n",
        "    - Devuelve UN string (respuesta + fuentes).\n",
        "    \"\"\"\n",
        "    # Lee parámetros actuales elegidos por el usuario\n",
        "    K = int(PARAMS[\"k\"])\n",
        "    TEMPERATURE = float(PARAMS[\"temperature\"])\n",
        "    TOP_P = float(PARAMS[\"top_p\"])\n",
        "    MAX_TOKENS = int(PARAMS[\"max_tokens\"])\n",
        "\n",
        "    # 1) Router: elegir índice de CV\n",
        "    candidates = list_cv_indices()\n",
        "    chosen_index = router_agent(user_message, candidates)\n",
        "\n",
        "    # 2) Retrieve desde el índice elegido (o mensaje si no hay)\n",
        "    if chosen_index:\n",
        "        matches = retrieve_from_index(chosen_index, user_message, top_k=K)\n",
        "        chosen_info = chosen_index\n",
        "    else:\n",
        "        matches = []\n",
        "        chosen_info = \"— (sin selección)\"\n",
        "\n",
        "    context_text = build_context(matches)\n",
        "\n",
        "    # 3) Armar mensajes para Groq\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    messages.append({\"role\": \"system\", \"content\": f\"Índice seleccionado por el router: {chosen_info}\"})\n",
        "    messages.append({\"role\": \"system\", \"content\": f\"Contexto recuperado:\\n\\n{context_text}\"})\n",
        "\n",
        "    for u, b in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": u})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": b})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # 4) Llamada Groq con parámetros elegidos\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",\n",
        "        messages=messages,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        max_tokens=MAX_TOKENS\n",
        "    )\n",
        "    bot_reply = resp.choices[0].message.content\n",
        "\n",
        "    # 5) Añadir pie de fuentes en el MISMO string\n",
        "    if matches:\n",
        "        fuentes = \"\\n\".join([f\"• {m['archivo']} (score={m['score']})\" for m in matches])\n",
        "    else:\n",
        "        fuentes = \"—\"\n",
        "    bot_reply_with_sources = (\n",
        "        f\"{bot_reply}\\n\\n---\\n\"\n",
        "        f\"**Router → índice seleccionado:** {chosen_info}\\n\"\n",
        "        f\"**Fuentes (pasajes recuperados):**\\n{fuentes}\"\n",
        "    )\n",
        "\n",
        "    # 6) Guardar historial (nuestra simulación de contexto)\n",
        "    history.append((user_message, bot_reply_with_sources))\n",
        "\n",
        "    return bot_reply_with_sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqalmkhiC5oY"
      },
      "source": [
        "#Memoria del Chat\n",
        "\n",
        "Y se actualiza en la última línea de la función chat():\n",
        "\n",
        "La memoria se acumula en esta variable global:\n",
        "\n",
        "Cada vez que el usuario envía un mensaje, se guarda el par (pregunta, respuesta) en history.\n",
        "\n",
        "Esta history se reutiliza en cada nuevo mensaje para simular “memoria del chat”.\n",
        "\n",
        "La historia se convierte en parte del prompt enviado al modelo LLM (Groq):\n",
        "\n",
        "for u, b in history:\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": u})\n",
        "    messages.append({\"role\": \"assistant\", \"content\": b})\n",
        "\n",
        "Esto permite que el modelo entienda el contexto conversacional anterior.\n",
        "\n",
        "Memoria del Chat\n",
        "\n",
        "El modelo que estás usando es: \"llama3-8b-8192\"\n",
        "\n",
        "Esto significa que su ventana de contexto es de 8192 tokens (máximo que puede procesar en cada mensaje).\n",
        "\n",
        "Todo lo siguiente consume tokens:\n",
        "\n",
        "System prompts (prompts iniciales)\n",
        "\n",
        "Texto recuperado por RAG (context_text)\n",
        "\n",
        "Historial (history)\n",
        "\n",
        "Nueva pregunta del usuario\n",
        "\n",
        "Si todo eso suma más de 8192 tokens, el modelo recortará automáticamente el prompt, y puede ignorar partes viejas del historial o contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pUFFzQ3SDCVA"
      },
      "outputs": [],
      "source": [
        "history = []  # [(user, bot), ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QyLTX3JOWh29"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any, Optional, Tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QoPl0_IGWSx9"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Versión extendida: Multiagente y agente único\n",
        "# =========================\n",
        "\n",
        "def router_agent_single(user_question: str, candidates: List[str]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Elige UN único índice de CV según la pregunta del usuario.\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return None\n",
        "    cand_list = \"\\n\".join(f\"- {c}\" for c in candidates)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": ROUTER_SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": f\"\\u00cdndices disponibles:\\n{cand_list}\\n\\nPregunta: {user_question}\\n\\nDevuelve SOLO un nombre EXACTO de la lista, o 'NONE' si ninguno aplica.\"}\n",
        "    ]\n",
        "    try:\n",
        "        r = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=messages,\n",
        "            temperature=0.0,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        choice = (r.choices[0].message.content or \"\").strip().splitlines()[0].strip()\n",
        "        return choice if choice.upper() != \"NONE\" and choice in candidates else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def router_agent_multi(user_question: str, candidates: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Devuelve una lista de índices relevantes según la pregunta del usuario.\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "    cand_list = \"\\n\".join(f\"- {c}\" for c in candidates)\n",
        "    system_prompt = (\n",
        "        \"Eres un enrutador inteligente. Tu tarea es elegir todos los índices de la lista que sean relevantes para la consulta del usuario.\\n\"\n",
        "        \"Responde con una lista separada por comas, usando exactamente los nombres tal como aparecen.\\n\"\n",
        "        \"Si ninguno aplica, responde 'NONE'.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"\\u00cdndices disponibles:\\n{cand_list}\\n\\nPregunta: {user_question}\\n\\nDevuelve una lista separada por comas o 'NONE'.\"}\n",
        "    ]\n",
        "    try:\n",
        "        r = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=messages,\n",
        "            temperature=0.0,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        raw = (r.choices[0].message.content or \"\").strip()\n",
        "        if raw.upper() == \"NONE\":\n",
        "            return []\n",
        "        selected = [s.strip() for s in raw.split(\",\") if s.strip() in candidates]\n",
        "        return selected\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def chat_multiagent(user_message: str, chat_history: List[Tuple[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    - Usa el router multiagente para detectar todos los CVs relevantes.\n",
        "    - Consulta a cada índice por separado y concatena el contexto.\n",
        "    - Genera una única respuesta compuesta con fuentes diferenciadas.\n",
        "    \"\"\"\n",
        "    K = int(PARAMS[\"k\"])\n",
        "    TEMPERATURE = float(PARAMS[\"temperature\"])\n",
        "    TOP_P = float(PARAMS[\"top_p\"])\n",
        "    MAX_TOKENS = int(PARAMS[\"max_tokens\"])\n",
        "\n",
        "    candidates = list_cv_indices()\n",
        "    selected_indices = router_agent_multi(user_message, candidates)\n",
        "\n",
        "    all_matches = []\n",
        "    all_contexts = []\n",
        "    index_labels = []\n",
        "\n",
        "    for idx in selected_indices:\n",
        "        matches = retrieve_from_index(idx, user_message, top_k=K)\n",
        "        context = build_context(matches)\n",
        "        all_matches.extend(matches)\n",
        "        index_labels.append(idx)\n",
        "        all_contexts.append(f\"## Contexto de {idx}\\n\\n{context}\")\n",
        "\n",
        "    combined_context = \"\\n\\n\".join(all_contexts) if all_contexts else \"(no se recuperó contexto)\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"system\", \"content\": f\"Índices seleccionados: {', '.join(index_labels) if index_labels else '— (ninguno)'}\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Contexto recuperado:\\n\\n{combined_context}\"}\n",
        "    ]\n",
        "\n",
        "    for u, b in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": u})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": b})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",\n",
        "        messages=messages,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        max_tokens=MAX_TOKENS\n",
        "    )\n",
        "    bot_reply = resp.choices[0].message.content\n",
        "\n",
        "    if all_matches:\n",
        "        fuentes = \"\\n\".join([f\"• {m['archivo']} (score={m['score']})\" for m in all_matches])\n",
        "    else:\n",
        "        fuentes = \"—\"\n",
        "\n",
        "    bot_reply_with_sources = (\n",
        "        f\"{bot_reply}\\n\\n---\\n\"\n",
        "        f\"**Router → índices seleccionados:** {', '.join(index_labels) if index_labels else '—'}\\n\"\n",
        "        f\"**Fuentes:**\\n{fuentes}\"\n",
        "    )\n",
        "\n",
        "    chat_history.append((user_message, bot_reply_with_sources))\n",
        "    return bot_reply_with_sources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "A8F6ywfHW1vY"
      },
      "outputs": [],
      "source": [
        "PARAMS = {\n",
        "    \"k\": 10,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.7,\n",
        "    \"max_tokens\": 512\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iOZh2JGYSX2",
        "outputId": "d7b9fa3e-2828-4406-a59c-a4bf9f4386fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexado OK. Archivos: 2 | Índices nuevos: 2 | Actualizados: 0 | Chunks insertados: 16\n"
          ]
        }
      ],
      "source": [
        "folder = \"/content/cv\"  # o donde tengas tus .pdf/.txt\n",
        "msg = upsert_documents_per_cv(folder)\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67uTPQtGXQug",
        "outputId": "b4640a80-172f-4533-9109-536264a2e1b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cv--cv1', 'cv--cv2']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_cv_indices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "F0a_cGZ3WSu_",
        "outputId": "c0da9a59-be01-4292-d1a7-72b7c7407d9c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Contexto de cv--cv2\\n\\nSofia Martinez:\\n\\n* Especialista en desarrollo de software, arquitecturas de datos e inteligencia artificial\\n* Experiencia en sistemas distribuidos, automatización de procesos empresariales mediante RPA y despliegue de modelos de IA con MLOps en entornos cloud (AWS, GCP)\\n* Diplomatura en Deep Learning - MIT\\n* Experiencia en startups y empresas multinacionales\\n* Enfoque en soluciones escalables, seguras y eficientes en la nube\\n\\n---\\n**Router → índices seleccionados:** cv--cv1, cv--cv2\\n**Fuentes:**\\n• cv1.txt (score=0.3898)\\n• cv1.txt (score=0.3887)\\n• cv1.txt (score=0.2385)\\n• cv1.txt (score=0.2352)\\n• cv2.txt (score=0.5111)\\n• cv2.txt (score=0.4835)\\n• cv2.txt (score=0.2532)\\n• cv2.txt (score=0.2049)'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_multiagent(\"resume el cv de sofia\", [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "1TvXwWyOWSpg",
        "outputId": "b5e9f300-53bd-4df8-b7a7-19ecce732ba6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Según el contexto, el CV de Ricardo Lopez es el siguiente:\\n\\n* Es un ejecutivo con experiencia en liderazgo empresarial y gestión de proyectos tecnológicos.\\n* Especialista en transformación digital y gestión de equipos multidisciplinarios.\\n* Enfoque en cumplimiento de KPIs y desarrollo de cultura basada en datos.\\n* Experiencia laboral como Director de Transformación Digital en TechCorp y Gerente de Proyectos en Empresa XYZ.\\n* Implementó estrategias de innovación y lideró equipos en telecomunicaciones y otros sectores.\\n* Ubicado en Buenos Aires, Argentina y tiene un perfil en LinkedIn.\\n\\n---\\n**Router → índices seleccionados:** cv--cv1, cv--cv2\\n**Fuentes:**\\n• cv1.txt (score=0.5742)\\n• cv1.txt (score=0.4811)\\n• cv1.txt (score=0.2956)\\n• cv1.txt (score=0.2463)\\n• cv2.txt (score=0.4253)\\n• cv2.txt (score=0.3512)\\n• cv2.txt (score=0.2441)\\n• cv2.txt (score=0.173)'"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_multiagent(\"resume el cv de ricardo\", [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "fGqU2np3WSfN",
        "outputId": "1e686c3a-bba0-471b-fef4-de3facc4ca9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Según el contexto, Ricardo Lopez (cv--cv1) tiene 47 años, mientras que Sofia Martinez (cv--cv2) tiene 38 años. Por lo tanto, Ricardo Lopez es el candidato más grande en edad.\\n\\n---\\n**Router → índices seleccionados:** cv--cv1, cv--cv2\\n**Fuentes:**\\n• cv1.txt (score=0.3716)\\n• cv1.txt (score=0.3489)\\n• cv1.txt (score=0.3201)\\n• cv1.txt (score=0.3084)\\n• cv2.txt (score=0.3146)\\n• cv2.txt (score=0.2824)\\n• cv2.txt (score=0.2813)\\n• cv2.txt (score=0.2049)'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_multiagent(\"Cual de los dos candidatos es mas grande en edad\", [])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rG-Bn4BZtV0"
      },
      "source": [
        "#Interfaz de usuario (Gradio)\n",
        "\n",
        "Incluye:\n",
        "\n",
        "Botón para reindexar.\n",
        "\n",
        "Sliders para k, temp, top_p, max_tokens.\n",
        "\n",
        "Chat interface.\n",
        "\n",
        "Herramienta para verificar chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "2N4nMCVBcG8G",
        "outputId": "bfd7bfb9-f969-4bc3-d72c-8f32b1e5106f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-150938962.py:80: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbox = gr.Chatbot()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://54eb75b60747bd00d9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://54eb75b60747bd00d9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =========================\n",
        "# Interfaz Gradio final — Multiagente con ejemplos y limpieza\n",
        "# =========================\n",
        "\n",
        "PARAMS = {\n",
        "    \"k\": TOP_K_DEFAULT,\n",
        "    \"temperature\": TEMPERATURE_DEFAULT,\n",
        "    \"top_p\": TOP_P_DEFAULT,\n",
        "    \"max_tokens\": MAX_TOKENS_DEFAULT,\n",
        "}\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "# Reindexar documentos\n",
        "\n",
        "def reindex_action():\n",
        "    os.makedirs(DOCS_DIR, exist_ok=True)\n",
        "    return upsert_documents_per_cv(DOCS_DIR)\n",
        "\n",
        "# Borrar todos los índices (para limpieza completa)\n",
        "def reset_all():\n",
        "    deleted = []\n",
        "    for idx in list_cv_indices():\n",
        "        try:\n",
        "            pc.delete_index(idx)\n",
        "            deleted.append(idx)\n",
        "        except Exception:\n",
        "            pass\n",
        "    chat_history.clear()\n",
        "    return f\"Se eliminaron los índices: {', '.join(deleted)}\\nHistorial reiniciado.\"\n",
        "\n",
        "# Borrar solo el historial\n",
        "\n",
        "def limpiar_chat():\n",
        "    chat_history.clear()\n",
        "    return [], \"\"\n",
        "\n",
        "# Callbacks sliders\n",
        "\n",
        "def _set_k(v):           PARAMS[\"k\"] = int(v)\n",
        "def _set_temp(v):        PARAMS[\"temperature\"] = float(v)\n",
        "def _set_top_p(v):       PARAMS[\"top_p\"] = float(v)\n",
        "def _set_max_tokens(v):  PARAMS[\"max_tokens\"] = int(v)\n",
        "\n",
        "# Interfaz\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## RAG multi-agente por CV — Groq + Pinecone\")\n",
        "    gr.Markdown(\"Subí tus CVs en `/content/cv` (.pdf o .txt) y presioná **Reindexar** para crear un índice por cada CV.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        re_btn = gr.Button(\"Reindexar (/content/cv)\")\n",
        "        reset_btn = gr.Button(\"🧹 Limpiar todo (historial + índices)\")\n",
        "    re_out = gr.Textbox(label=\"Estado\", lines=3)\n",
        "    re_btn.click(fn=reindex_action, outputs=[re_out])\n",
        "    reset_btn.click(fn=reset_all, outputs=[re_out])\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### Parámetros de recuperación y generación\")\n",
        "\n",
        "    with gr.Row():\n",
        "        k_slider = gr.Slider(label=\"Top-K retrieve\", minimum=1, maximum=10, step=1, value=PARAMS[\"k\"])\n",
        "        temp_slider = gr.Slider(label=\"Temperature\", minimum=0.0, maximum=1.5, step=0.1, value=PARAMS[\"temperature\"])\n",
        "        top_p_slider = gr.Slider(label=\"Top-p\", minimum=0.1, maximum=1.0, step=0.05, value=PARAMS[\"top_p\"])\n",
        "        max_tk_slider = gr.Slider(label=\"Max tokens\", minimum=64, maximum=2048, step=32, value=PARAMS[\"max_tokens\"])\n",
        "\n",
        "    k_slider.change(_set_k, inputs=[k_slider])\n",
        "    temp_slider.change(_set_temp, inputs=[temp_slider])\n",
        "    top_p_slider.change(_set_top_p, inputs=[top_p_slider])\n",
        "    max_tk_slider.change(_set_max_tokens, inputs=[max_tk_slider])\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### Modo de operación\")\n",
        "    modo_multi = gr.Checkbox(label=\"Usar modo multiagente (varios CVs) 💡\", value=True)\n",
        "\n",
        "    gr.Markdown(\"### Chat con el sistema\")\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(label=\"Tu pregunta\", lines=2, scale=8)\n",
        "        send_btn = gr.Button(\"Enviar\", scale=2)\n",
        "\n",
        "    chatbox = gr.Chatbot()\n",
        "    clear_btn = gr.Button(\"🧼 Limpiar chat\")\n",
        "\n",
        "    # Ejemplos\n",
        "    ejemplos = [\n",
        "        \"¿En qué proyectos de ciencia de datos participó Ricardo?\",\n",
        "        \"¿Qué experiencia tienen Sofía y Ricardo con sklearn, XGBoost o LightGBM?\",\n",
        "        \"¿Cuál de los candidatos tiene un perfil más académico?\",\n",
        "        \"¿Cuál de los candidatos tiene un perfil más corporativo?\",\n",
        "        \"¿Quién de los candidatos sabe Python y trabajó en minería?\",\n",
        "        \"¿Qué personas tienen experiencia en proyectos internacionales y manejo de AWS o GCP?\",\n",
        "        \"Estoy buscando alguien con conocimientos en ML, datos y experiencia minera. ¿A quién considero?\",\n",
        "        \"¿Quiénes lideraron equipos técnicos y usaron Power BI o Tableau?\"\n",
        "    ]\n",
        "    gr.Examples(examples=ejemplos, inputs=msg)\n",
        "\n",
        "    # Lógica de respuesta\n",
        "    def responder(input_text, modo_multi):\n",
        "          if not input_text.strip():\n",
        "              return []\n",
        "\n",
        "          print(\"🔧 Parámetros actuales:\")\n",
        "          print(PARAMS)\n",
        "\n",
        "          if modo_multi:\n",
        "              respuesta = chat_multiagent(input_text, chat_history)\n",
        "          else:\n",
        "              respuesta = chat(input_text, chat_history)\n",
        "\n",
        "          return [(input_text, respuesta)]\n",
        "\n",
        "    send_btn.click(fn=responder, inputs=[msg, modo_multi], outputs=[chatbox])\n",
        "    msg.submit(fn=responder, inputs=[msg, modo_multi], outputs=[chatbox]).then(\n",
        "        fn=lambda: \"\", inputs=None, outputs=msg\n",
        "    )\n",
        "\n",
        "    clear_btn.click(fn=limpiar_chat, outputs=[chatbox, msg])\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### Vista de chunks iniciales\")\n",
        "    verif_btn = gr.Button(\"Verificar primer chunk de cada CV\")\n",
        "    verif_out = gr.Textbox(label=\"Preview de chunks\", lines=10)\n",
        "    verif_btn.click(fn=verificar_chunks, outputs=[verif_out])\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4CLI7O2Zp_b"
      },
      "source": [
        "#DEBUGGING VERIFICACION MANUAL\n",
        "\n",
        "Es para ejecutar paso a paso todo el flujo con un archivo específico, esto para analizar los errores que fueron surgiendo:\n",
        "\n",
        "Lee archivo.\n",
        "\n",
        "Divide en chunks.\n",
        "\n",
        "Calcula embeddings.\n",
        "\n",
        "Hace upsert en Pinecone.\n",
        "\n",
        "Elimina índices si es necesario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x0pMW5-2GgB",
        "outputId": "966b7cc6-568d-4999-cbdf-6bdbbae51fa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto leído:\n",
            "('# CV Corporativo - Ricardo Lopez ## Datos Personales - Nombre: Ricardo Lopez '\n",
            " '- Edad: 47 anios - Correo: ricardo.lopez.corporativo@example.com - '\n",
            " 'Ubicacion: Buenos Aires, Argentina - LinkedIn: '\n",
            " 'linkedin.com/in/ricardolopez-corporativo ## Resumen Profesional - Ejecutivo '\n",
            " 'con experiencia en liderazgo empresarial y gestion de proyectos '\n",
            " 'tecnologicos. - Especialista en transformacion digital y gestion de equipos '\n",
            " 'multidisciplinarios. - Enfoque en cumplimiento de KPIs y desarrollo de '\n",
            " 'cultura basada en datos. ## Experiencia Laboral - **Director de '\n",
            " 'Transformacion Digital en TechCorp**: implementacion de estrategias de '\n",
            " 'innovacion. - **Gerente de Proyectos en Empresa XYZ**: liderazgo de equipos '\n",
            " 'en telecomunicaciones y finanzas. - **Consultor Senior en Deloitte**: '\n",
            " 'asesoria en analitica de datos y gestion de riesgos. ## Logros Destacados - '\n",
            " 'Reduccion de costos operativos en un 20% mediante procesos de '\n",
            " 'digitalizacion. - Implementacion de tableros de control para seguimiento de '\n",
            " 'indicadores clave. - Desarr')\n",
            "\n",
            "Chunks generados: 8\n",
            "--- Chunk 1 ---\n",
            "# CV Corporativo - Ricardo Lopez ## Datos Personales - Nombre: Ricardo Lopez - Edad: 47 anios - Correo: ricardo.lopez.corporativo@example.com - Ubicacion: Buenos Aires, Argentina - LinkedIn:\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Argentina - LinkedIn: linkedin.com/in/ricardolopez-corporativo ## Resumen Profesional - Ejecutivo con experiencia en liderazgo empresarial y gestion de proyectos tecnologicos. - Especialista en\n",
            "\n",
            "--- Chunk 3 ---\n",
            "- Especialista en transformacion digital y gestion de equipos multidisciplinarios. - Enfoque en cumplimiento de KPIs y desarrollo de cultura basada en datos. ## Experiencia Laboral - **Director de\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "archivo = \"/content/cv/cv1.txt\"\n",
        "text = read_txt(archivo)\n",
        "print(\"Texto leído:\")\n",
        "pprint(text[:1000])\n",
        "\n",
        "chunks = chunk_text(text)\n",
        "print(f\"\\nChunks generados: {len(chunks)}\")\n",
        "for i, c in enumerate(chunks[:3]):\n",
        "    print(f\"--- Chunk {i+1} ---\\n{c}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ck2erb2pOj",
        "outputId": "d046470d-222f-4395-db9f-1664ca099d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/cv/!file:   cannot open `/content/cv/!file' (No such file or directory)\n",
            "/content/cv/cv1.txt: text/plain; charset=us-ascii\n",
            "/content/cv/cv2.txt: text/plain; charset=us-ascii\n"
          ]
        }
      ],
      "source": [
        "!file -i /content/cv/!file -i /content/cv/*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIHkQlyb4zMO",
        "outputId": "28967ffc-fc35-44ea-a464-3f4882c86bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# CV Corporativo - Ricardo Lopez\n",
            "\n",
            "## Datos Personales\n",
            "- Nombre: Ricardo Lopez  \n",
            "- Edad: 47 anios  \n",
            "- Correo: ricardo.lopez.corporativo@example.com  \n",
            "- Ubicacion: Buenos Aires, Argentina  \n",
            "- LinkedIn: linkedin.com/in/ricardolopez-corporativo  \n",
            "\n",
            "## Resumen Profesional\n",
            "- Ejecutivo con experiencia en liderazgo empresarial y gestion de proyectos tecnologicos.  \n",
            "- Especialista en transformacion digital y gestion de equipos multidisciplinarios.  \n",
            "- Enfoque en cumplimiento de KPIs y desarrollo de cultur\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/cv/cv1.txt\", encoding=\"latin-1\") as f:\n",
        "    print(f.read()[:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hJTXBeB49jx",
        "outputId": "51ed701d-c816-4fdf-d41c-de325d7aed80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8 vectores de embeddings generados.\n",
            "[-0.02830718830227852, -0.03697291761636734, -0.043179091066122055, 0.014025023207068443, 0.09206068515777588]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    vecs = embed_texts(chunks)\n",
        "    print(f\"{len(vecs)} vectores de embeddings generados.\")\n",
        "    print(vecs[0][:5])  # Mostrar los primeros 5 valores del primer vector\n",
        "except Exception as e:\n",
        "    print(f\"Error al generar embeddings: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We8ncrJ96Ga9",
        "outputId": "c440509f-a19b-4a2d-8a2c-995791ec7981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upserting batch 0–200 en índice 'cv--cv1'\n",
            "Error al hacer upsert en Pinecone: 'NoneType' object is not callable\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    BATCH = 200\n",
        "    for i in range(0, len(vectors), BATCH):\n",
        "        print(f\"Upserting batch {i}–{i+BATCH} en índice '{index_name}'\")\n",
        "        idx.upsert(vectors=vectors[i:i+BATCH])\n",
        "except Exception as e:\n",
        "    print(f\"Error al hacer upsert en Pinecone: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "idHsmYuL7021"
      },
      "outputs": [],
      "source": [
        "indices = pc.list_indexes()\n",
        "for idx in indices:\n",
        "    nombre = idx[\"name\"]\n",
        "    if nombre.startswith(\"cv-\"):\n",
        "        pc.delete_index(nombre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OwRFHw76gSp",
        "outputId": "37e4e46b-6ae6-40bc-91d1-6f568f3a4065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Procesando archivo: cv1.txt\n",
            "Índice: cv--cv1\n",
            "8 chunks generados\n",
            "8 vectores generados\n",
            "Subiendo batch 0–200\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "path = Path(\"/content/cv/cv1.txt\")\n",
        "\n",
        "base_noext = path.stem\n",
        "index_name = sanitize_index_name(base_noext)\n",
        "\n",
        "print(f\"\\n Procesando archivo: {path.name}\")\n",
        "print(f\"Índice: {index_name}\")\n",
        "\n",
        "ensure_index(index_name)\n",
        "\n",
        "raw_text = read_file(path).strip()\n",
        "if not raw_text:\n",
        "    print(f\"Vacío o ilegible: {path.name}\")\n",
        "else:\n",
        "    chunks = chunk_text(raw_text)\n",
        "    if not chunks:\n",
        "        print(f\"Sin chunks: {path.name}\")\n",
        "    else:\n",
        "        print(f\"{len(chunks)} chunks generados\")\n",
        "        try:\n",
        "            vecs = embed_texts(chunks)\n",
        "            print(f\"{len(vecs)} vectores generados\")\n",
        "            vectors = []\n",
        "            for i, vec in enumerate(vecs):\n",
        "                vid = f\"{base_noext}-{i:04d}\"\n",
        "                vectors.append({\n",
        "                    \"id\": vid,\n",
        "                    \"values\": vec,\n",
        "                    \"metadata\": {\n",
        "                        \"texto\": chunks[i],\n",
        "                        \"archivo\": path.name,\n",
        "                        \"chunk_id\": i,\n",
        "                        \"fecha\": datetime.today().strftime(\"%Y-%m-%d\")\n",
        "                    }\n",
        "                })\n",
        "\n",
        "            idx = pc.Index(index_name)\n",
        "            for i in range(0, len(vectors), 200):\n",
        "                print(f\"Subiendo batch {i}–{i+200}\")\n",
        "                idx.upsert(vectors=vectors[i:i+200])\n",
        "        except Exception as e:\n",
        "            print(f\"Error al generar embeddings o subir: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
