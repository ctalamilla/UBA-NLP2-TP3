{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuhbCKQl3iQE"
      },
      "source": [
        "#Instalacion de las librer√≠as necesarias:\n",
        "\n",
        "gradio: UI web para el chat.\n",
        "\n",
        "groq: API cliente del modelo LLM de Groq.\n",
        "\n",
        "pinecone-client: para vectores e √≠ndices.\n",
        "\n",
        "sentence-transformers: para generar embeddings.\n",
        "\n",
        "langchain-text-splitters: para dividir texto.\n",
        "\n",
        "pypdf: para leer PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IXCRYSFXvxM",
        "outputId": "5d82922a-4e9e-43ca-fd0f-96878198d406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install gradio groq pinecone-client sentence-transformers langchain-text-splitters pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9upZEWaAeU22",
        "outputId": "375647ec-e406-40ca-a7ac-feb38f494180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "3wFutQTCei6u",
        "outputId": "61f2e6a7-5d31-47d9-a0a0-7334be1d6cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pinecone\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.8.3)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.14.1)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Collecting packaging<25.0,>=24.2 (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging, pinecone-plugin-assistant, pinecone\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "Successfully installed packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.7.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "6836147c71454643a5375fb782cd0bdc",
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2ddgUQR97-eE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import gradio as gr\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "from pinecone import (\n",
        "    Pinecone,\n",
        "    ServerlessSpec,\n",
        "    CloudProvider,\n",
        "    AwsRegion,\n",
        "    VectorType\n",
        ")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from pypdf import PdfReader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ybr3gHG_0BW"
      },
      "source": [
        "# Configuruacion y testeos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoGlaxMS4VSS"
      },
      "source": [
        "Obtiene las API keys desde los \"secrets\" de Colab. Si no existen, lanza un error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXkcNdcN4Rir",
        "outputId": "0bb1122b-904e-4e5d-d8d9-c0bcc8094f2a"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Secrets (Colab) - con fallback a input\n",
        "# =========================\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "def get_secret_or_input(key_name: str, prompt: str) -> str:\n",
        "    \"\"\"Intenta obtener un secreto de Colab.\n",
        "    Si no existe, lo pide por consola.\"\"\"\n",
        "    try:\n",
        "        value = userdata.get(key_name)\n",
        "        if value:\n",
        "            return value.strip()\n",
        "    except Exception:\n",
        "        # SecretNotFoundError u otro\n",
        "        pass\n",
        "    # Si no est√° en userdata, lo pide manualmente\n",
        "    return input(f\"üëâ Ingres√° tu {prompt}: \").strip()\n",
        "\n",
        "GROQ_API_KEY = get_secret_or_input(\"GROQ_API_KEY\", \"GROQ_API_KEY\")\n",
        "PINECONE_API_KEY = get_secret_or_input(\"PINECONE_API_KEY\", \"PINECONE_API_KEY\")\n",
        "\n",
        "# Guardar en variables de entorno\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "print(\"‚úÖ Claves configuradas correctamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-yzFSZBAO3Y",
        "outputId": "97c31b59-a20e-4e42-dd1c-1f8f4dbb5408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GROQ OK: OK GROQ\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Test GROQ\n",
        "# =========================\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "assert os.environ.get(\"GROQ_API_KEY\"), \"‚ùå Falta GROQ_API_KEY en os.environ\"\n",
        "\n",
        "try:\n",
        "    client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "    chat = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Sos un asistente muy conciso.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Decime 'OK GROQ' si me le√©s bien.\"}\n",
        "        ],\n",
        "        max_tokens=8,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    print(\"‚úÖ GROQ OK:\", chat.choices[0].message.content.strip())\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error GROQ:\", type(e).__name__, str(e))\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r1lDc9HEARZW"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import os, time, uuid, numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AIaEzO-ko8wj"
      },
      "outputs": [],
      "source": [
        "def pinecone_reset_all(api_key=None, *, realmente_borrar=False, wait=True, timeout=120, poll_every=2.0):\n",
        "    \"\"\"\n",
        "    Borra TODOS los √≠ndices de Pinecone (irreversible).\n",
        "    - Si `realmente_borrar=False` (por defecto), solo muestra qu√© borrar√≠a (dry-run).\n",
        "    - Si `wait=True`, espera a que desaparezcan del listado (hasta `timeout` segundos).\n",
        "\n",
        "    Uso:\n",
        "      # ver qu√© hay (dry-run)\n",
        "      pinecone_reset_all()\n",
        "\n",
        "      # borrar de verdad\n",
        "      pinecone_reset_all(realmente_borrar=True)\n",
        "    \"\"\"\n",
        "    import os, time\n",
        "    from pinecone import Pinecone\n",
        "\n",
        "    # Obtener API key (env o Colab)\n",
        "    api_key = api_key or os.getenv(\"PINECONE_API_KEY\")\n",
        "    if not api_key:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Falta PINECONE_API_KEY (en entorno o en Colab > Secrets).\")\n",
        "\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "\n",
        "    items = pc.list_indexes()\n",
        "\n",
        "    # Compatibilidad con distintas formas de listado\n",
        "    def _name(x):\n",
        "        if isinstance(x, str): return x\n",
        "        n = getattr(x, \"name\", None)\n",
        "        if isinstance(n, str): return n\n",
        "        if isinstance(x, dict): return x.get(\"name\")\n",
        "        return None\n",
        "\n",
        "    names = [n for n in map(_name, items) if n]\n",
        "\n",
        "    if not realmente_borrar:\n",
        "        return {\"accion\": \"dry-run\", \"encontrados\": names}\n",
        "\n",
        "    for n in names:\n",
        "        pc.delete_index(n)\n",
        "\n",
        "    if not wait:\n",
        "        return {\"accion\": \"deleted\", \"borrados\": names}\n",
        "\n",
        "    # Polling hasta que no aparezcan\n",
        "    t0 = time.time()\n",
        "    remaining = names[:]\n",
        "    while time.time() - t0 < timeout:\n",
        "        now = [n for n in map(_name, pc.list_indexes()) if n]\n",
        "        remaining = [n for n in remaining if n in now]\n",
        "        if not remaining:\n",
        "            break\n",
        "        time.sleep(poll_every)\n",
        "\n",
        "    return {\"accion\": \"deleted\", \"borrados\": names, \"restantes\": remaining}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RR1mqGkpC_E",
        "outputId": "785b1166-91a1-4385-bd66-08a1fb2e7cd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accion': 'dry-run', 'encontrados': ['cv--cv1', 'cv--cv2']}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1) Ver qu√© √≠ndices hay (no borra)\n",
        "pinecone_reset_all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRdBQdeSpJJB",
        "outputId": "8a8b64b4-df7e-4812-80c6-4601c2c06225"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accion': 'deleted', 'borrados': ['cv--cv1', 'cv--cv2'], 'restantes': []}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2) Borrar todo y esperar confirmaci√≥n\n",
        "pinecone_reset_all(realmente_borrar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFVzSWP3pL0B",
        "outputId": "24c94554-5834-48c8-dc4e-151cd1876537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accion': 'dry-run', 'encontrados': []}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1) Ver qu√© √≠ndices hay (no borra)\n",
        "pinecone_reset_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gus3ypi37yi"
      },
      "source": [
        "#Definicion:\n",
        "\n",
        "Directorio de documentos (/content/cv)\n",
        "\n",
        "Modelo de embeddings (MiniLM)\n",
        "\n",
        "Tama√±o y solapamiento de chunks\n",
        "\n",
        "Regi√≥n y proveedor para Pinecone\n",
        "\n",
        "Par√°metros default para generaci√≥n (top_p, temperature, max_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p90mH4m6ErM6"
      },
      "outputs": [],
      "source": [
        "# Carpeta destino\n",
        "!mkdir -p /content/cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsKBuQtoAtdo",
        "outputId": "f9452fba-4b0f-465e-f0ab-0496f01ed3c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-08-24 13:50:34--  https://raw.githubusercontent.com/ctalamilla/UBA-NLP2-TP3/refs/heads/main/cv2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1389 (1.4K) [text/plain]\n",
            "Saving to: ‚Äò/content/cv/cv2.txt‚Äô\n",
            "\n",
            "\rcv2.txt               0%[                    ]       0  --.-KB/s               \rcv2.txt             100%[===================>]   1.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-24 13:50:34 (17.3 MB/s) - ‚Äò/content/cv/cv2.txt‚Äô saved [1389/1389]\n",
            "\n",
            "--2025-08-24 13:50:34--  https://raw.githubusercontent.com/ctalamilla/UBA-NLP2-TP3/refs/heads/main/cv1.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1405 (1.4K) [text/plain]\n",
            "Saving to: ‚Äò/content/cv/cv1.txt‚Äô\n",
            "\n",
            "cv1.txt             100%[===================>]   1.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-24 13:50:35 (20.0 MB/s) - ‚Äò/content/cv/cv1.txt‚Äô saved [1405/1405]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Lista de URLs (una por l√≠nea)\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/ctalamilla/UBA-NLP2-TP3/refs/heads/main/cv2.txt\",\n",
        "    \"https://raw.githubusercontent.com/ctalamilla/UBA-NLP2-TP3/refs/heads/main/cv1.txt\"\n",
        "    ]\n",
        "\n",
        "# Descargar todos en la carpeta /content/cv\n",
        "for u in urls:\n",
        "    !wget -nc -P /content/cv \"{u}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Q-qxS4mo7-26"
      },
      "outputs": [],
      "source": [
        "DOCS_DIR = \"/content/cv\"                   # (.pdf / .txt)\n",
        "INDEX_PREFIX = \"cv--\"                       # Prefijo para √≠ndices por CV\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "CHUNK_SIZE = 200\n",
        "CHUNK_OVERLAP = 25\n",
        "EMB_DIM = 384                              # all-MiniLM-L6-v2 -> 384\n",
        "PINECONE_CLOUD = CloudProvider.AWS\n",
        "PINECONE_REGION = AwsRegion.US_EAST_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-t6TQ3Vg7-6C"
      },
      "outputs": [],
      "source": [
        "# Par√°metros de generaci√≥n por defecto (Se ajustan desde la interfase)\n",
        "TOP_K_DEFAULT = 4\n",
        "TEMPERATURE_DEFAULT = 0.7\n",
        "TOP_P_DEFAULT = 0.9\n",
        "MAX_TOKENS_DEFAULT = 512\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Eres un asistente que responde usando EXCLUSIVAMENTE el contexto proporcionado. \"\n",
        "    \"Si el contexto no contiene la respuesta, dilo con claridad. S√© breve, claro y √∫til.\"\n",
        ")\n",
        "\n",
        "ROUTER_SYSTEM = (\n",
        "    \"Eres un enrutador. Tu tarea es elegir UN √öNICO √≠ndice de la lista que mejor \"\n",
        "    \"corresponda a la consulta del usuario. Responde SOLO con el nombre EXACTO del √≠ndice y nada m√°s. \"\n",
        "    \"Si ninguno aplica, responde 'NONE'.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "JZfUCnu6HNDz"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Groq client\n",
        "# =========================\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# =========================\n",
        "# Pinecone client\n",
        "# =========================\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "s8HHVuNY4kpX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUND2oUW4sLN"
      },
      "source": [
        "#Embeddings y Chunking\n",
        "\n",
        "Divide texto largo en chunks superpuestos.\n",
        "\n",
        "Calcula los embeddings de cada chunk (o uno solo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ZWIj6TJmHTLu"
      },
      "outputs": [],
      "source": [
        "# Embeddings\n",
        "emb_model = SentenceTransformer(EMB_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5VVlRGde40PQ"
      },
      "outputs": [],
      "source": [
        "# Chunking\n",
        "def chunk_text(text: str) -> List[str]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        separators=[\" \", \"\"]\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    return [vec.tolist() for vec in emb_model.encode(texts)]\n",
        "\n",
        "def embed_text(text: str) -> List[float]:\n",
        "    return emb_model.encode([text])[0].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiGBWUfa48si"
      },
      "source": [
        "#Limpieza del texto\n",
        "\n",
        "Elimina tildes y di√©resis (usando unicodedata.normalize).\n",
        "\n",
        "Convierte la √± a n.\n",
        "\n",
        "Elimina caracteres no ASCII y nulos.\n",
        "\n",
        "Reduce espacios m√∫ltiples a uno solo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "99hyKHTw5IOZ"
      },
      "outputs": [],
      "source": [
        "# Limpieza del texto\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    # Reemplaza caracteres acentuados por sus equivalentes no acentuados\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    text = \"\".join([c for c in text if not unicodedata.combining(c)])\n",
        "    # Reemplazar √± y √ë por n y N\n",
        "    text = text.replace(\"√±\", \"n\").replace(\"√ë\", \"N\")\n",
        "    # Eliminar caracteres no ASCII\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    # Eliminar nulos\n",
        "    text = text.replace(\"\\x00\", \"\")\n",
        "    # Colapsar m√∫ltiples espacios\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_PpuUD66-_k"
      },
      "source": [
        "#Lectura de archivos\n",
        "\n",
        "Lee archivos .txt como latin-1.\n",
        "\n",
        "Lee PDFs usando PdfReader.\n",
        "\n",
        "Usa clean_text() en ambos casos.\n",
        "\n",
        "read_file() elige el m√©todo seg√∫n la extensi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "gyXfl8Kfm33J"
      },
      "outputs": [],
      "source": [
        "# Lectura de archivos\n",
        "\n",
        "def read_txt(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
        "        raw = f.read()\n",
        "        return clean_text(raw)\n",
        "\n",
        "def read_pdf(path: str) -> str:\n",
        "    text_parts = []\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        for page in reader.pages:\n",
        "            text_parts.append(page.extract_text() or \"\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return clean_text(\"\\n\".join(text_parts))\n",
        "\n",
        "def read_file(path: Path) -> str:\n",
        "    ext = path.suffix.lower()\n",
        "    if ext == \".txt\":\n",
        "        return read_txt(str(path))\n",
        "    elif ext == \".pdf\":\n",
        "        return read_pdf(str(path))\n",
        "    return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "KYB_teFuJl4h",
        "outputId": "5f82e127-1035-4ab2-f2ec-d0ccfe95c3bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# CV Corporativo - Ricardo Lopez ## Datos Personales - Nombre: Ricardo Lopez - Edad: 47 anios - Correo: ricardo.lopez.corporativo@example.com - Ubicacion: Buenos Aires, Argentina - LinkedIn: linkedin.com/in/ricardolopez-corporativo ## Resumen Profesional - Ejecutivo con experiencia en liderazgo empresarial y gestion de proyectos tecnologicos. - Especialista en transformacion digital y gestion de equipos multidisciplinarios. - Enfoque en cumplimiento de KPIs y desarrollo de cultura basada en datos. ## Experiencia Laboral - **Director de Transformacion Digital en TechCorp**: implementacion de estrategias de innovacion. - **Gerente de Proyectos en Empresa XYZ**: liderazgo de equipos en telecomunicaciones y finanzas. - **Consultor Senior en Deloitte**: asesoria en analitica de datos y gestion de riesgos. ## Logros Destacados - Reduccion de costos operativos en un 20% mediante procesos de digitalizacion. - Implementacion de tableros de control para seguimiento de indicadores clave. - Desarrollo de programas de capacitacion en gestion agil y data-driven. ## Educacion - MBA - Universidad Austral. - Maestria en Ciencia de Datos - UBA. - Ingenieria en Sistemas - Universidad Nacional. ## Habilidades - Gestion de proyectos, liderazgo, transformacion digital. - Power BI, Tableau, SQL, Python. - Negociacion con stakeholders y gestion de presupuestos.'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_text(read_txt('/content/cv/cv1.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULcnMWx579XP"
      },
      "source": [
        "#Gesti√≥n de √≠ndices de Pinecone\n",
        "\n",
        "Convierte nombres de archivo en nombres v√°lidos para √≠ndices Pinecone.\n",
        "\n",
        "Lista los √≠ndices existentes.\n",
        "\n",
        "Crea un √≠ndice si no existe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "uyVcEybc_d7l"
      },
      "outputs": [],
      "source": [
        "# Helpers de √≠ndices por CV\n",
        "\n",
        "def sanitize_index_name(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Pinecone index name: lowercase, alfanum√©rico y guiones, 1-45 chars, comenzar con letra.\n",
        "    Creamos: f\"cv_{slug}\" donde slug se arma desde el nombre del archivo (sin extensi√≥n).\n",
        "    \"\"\"\n",
        "    slug = name.lower()\n",
        "    slug = re.sub(r\"\\s+\", \"-\", slug)\n",
        "    slug = re.sub(r\"[^a-z0-9-]\", \"-\", slug)\n",
        "    slug = re.sub(r\"-{2,}\", \"-\", slug).strip(\"-\")\n",
        "    if not slug or not slug[0].isalpha():\n",
        "        slug = \"a\" + slug\n",
        "    slug = slug[:40]  # dejamos margen para \"cv_\"\n",
        "    return f\"{INDEX_PREFIX}{slug}\"\n",
        "\n",
        "def list_cv_indices() -> List[str]:\n",
        "    listing = pc.list_indexes()\n",
        "    names = [item[\"name\"] for item in listing] if isinstance(listing, list) else [d.get(\"name\") for d in listing]\n",
        "    return sorted([n for n in names if n and n.startswith(INDEX_PREFIX)])\n",
        "\n",
        "def ensure_index(index_name: str):\n",
        "    existing = list_cv_indices()\n",
        "    if index_name not in existing:\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=EMB_DIM,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(\n",
        "                cloud=PINECONE_CLOUD,\n",
        "                region=PINECONE_REGION\n",
        "            ),\n",
        "            vector_type=VectorType.DENSE\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVQYBH1Z_Vk5"
      },
      "source": [
        "#Carga de documentos a Pinecone\n",
        "\n",
        "Para cada .txt o .pdf:\n",
        "\n",
        "Limpia y divide el texto.\n",
        "\n",
        "Calcula embeddings.\n",
        "\n",
        "Crea vectores con metadatos.\n",
        "\n",
        "Hace upsert (insertar o actualizar) en el √≠ndice correspondiente (uno por archivo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "uz5zaXB8_hwG"
      },
      "outputs": [],
      "source": [
        "# Ingesta/Upsert: UN √≠ndice por CV\n",
        "\n",
        "def upsert_documents_per_cv(folder: str) -> str:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    paths = [Path(p) for p in glob.glob(os.path.join(folder, \"*.pdf\"))] + \\\n",
        "            [Path(p) for p in glob.glob(os.path.join(folder, \"*.txt\"))]\n",
        "    if not paths:\n",
        "        return \"No se encontraron documentos (.pdf/.txt) en la carpeta.\"\n",
        "\n",
        "    total_chunks = 0\n",
        "    created = 0\n",
        "    updated = 0\n",
        "\n",
        "    for path in paths:\n",
        "        base_noext = path.stem\n",
        "        index_name = sanitize_index_name(base_noext)\n",
        "\n",
        "        # Asegurar √≠ndice para este CV\n",
        "        before = set(list_cv_indices())\n",
        "        ensure_index(index_name)\n",
        "        after = set(list_cv_indices())\n",
        "        if index_name in (after - before):\n",
        "            created += 1\n",
        "        else:\n",
        "            updated += 1\n",
        "\n",
        "        # Leer y trocear\n",
        "        raw_text = read_file(path).strip()\n",
        "        if not raw_text:\n",
        "            print(f\"Vac√≠o o ilegible: {path.name}\")\n",
        "            continue\n",
        "\n",
        "        chunks = chunk_text(raw_text)\n",
        "        if not chunks:\n",
        "            print(f\"Sin chunks: {path.name}\")\n",
        "            continue\n",
        "\n",
        "        vecs = embed_texts(chunks)\n",
        "        vectors = []\n",
        "        for i, vec in enumerate(vecs):\n",
        "            vid = f\"{base_noext}-{i:04d}\"\n",
        "            vectors.append({\n",
        "                \"id\": vid,\n",
        "                \"values\": vec,\n",
        "                \"metadata\": {\n",
        "                    \"texto\": chunks[i],\n",
        "                    \"archivo\": path.name,\n",
        "                    \"chunk_id\": i,\n",
        "                    \"fecha\": datetime.today().strftime(\"%Y-%m-%d\")\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Upsert al √≠ndice del CV\n",
        "        idx = pc.Index(index_name)\n",
        "        # Upsert por lotes\n",
        "        BATCH = 200\n",
        "        for i in range(0, len(vectors), BATCH):\n",
        "            idx.upsert(vectors=vectors[i:i+BATCH])\n",
        "\n",
        "        total_chunks += len(chunks)\n",
        "\n",
        "    return f\"Indexado OK. Archivos: {len(paths)} | √çndices nuevos: {created} | Actualizados: {updated} | Chunks insertados: {total_chunks}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwOLiIK5_mJd"
      },
      "source": [
        "#Consulta por vector (Retrieve)\n",
        "\n",
        "Usa el vector de la query para buscar en un √≠ndice de Pinecone.\n",
        "\n",
        "Devuelve los top_k matches.\n",
        "\n",
        "Arma texto con los resultados y sus fuentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Nei6ThGE_rvG"
      },
      "outputs": [],
      "source": [
        "# Retrieve (consulta vectorial) desde un √≠ndice concreto\n",
        "\n",
        "def retrieve_from_index(index_name: str, query: str, top_k: int = 4) -> List[Dict[str, Any]]:\n",
        "    idx = pc.Index(index_name)\n",
        "    qvec = embed_text(query)\n",
        "    res = idx.query(vector=qvec, top_k=int(top_k), include_metadata=True)\n",
        "\n",
        "    matches = res.get(\"matches\", []) if isinstance(res, dict) else getattr(res, \"matches\", [])\n",
        "    out = []\n",
        "    for m in matches or []:\n",
        "        score = m.get(\"score\") if isinstance(m, dict) else getattr(m, \"score\", None)\n",
        "        meta = m.get(\"metadata\", {}) if isinstance(m, dict) else getattr(m, \"metadata\", {}) or {}\n",
        "        out.append({\n",
        "            \"score\": round(float(score or 0.0), 4),\n",
        "            \"texto\": meta.get(\"texto\", \"\"),\n",
        "            \"archivo\": meta.get(\"archivo\", \"\")\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def build_context(matches: List[Dict[str, Any]]) -> str:\n",
        "    if not matches:\n",
        "        return \"(no se recuperaron pasajes relevantes)\"\n",
        "    parts = []\n",
        "    for i, m in enumerate(matches, 1):\n",
        "        parts.append(f\"[{i}] ({m['archivo']}, score={m['score']})\\n{m['texto']}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIWw4Kk9_vpe"
      },
      "source": [
        "#Router multiagente\n",
        "\n",
        "Usa un LLM (Groq) para decidir cu√°l √≠ndice (archivo) es m√°s relevante para una pregunta.\n",
        "\n",
        "Solo devuelve el nombre del √≠ndice (o NONE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "YdU41Zbqm_hy"
      },
      "outputs": [],
      "source": [
        "# Router Agent: elegir √çNDICE del CV\n",
        "\n",
        "def router_agent(user_question: str, candidates: List[str]) -> Optional[str]:\n",
        "    if not candidates:\n",
        "        return None\n",
        "    cand_list = \"\\n\".join(f\"- {c}\" for c in candidates)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": ROUTER_SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": f\"√çndices disponibles:\\n{cand_list}\\n\\nPregunta: {user_question}\\n\\nDevuelve SOLO un nombre EXACTO de la lista, o 'NONE' si ninguno aplica.\"}\n",
        "    ]\n",
        "    try:\n",
        "        r = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=messages,\n",
        "            temperature=0.0,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        choice = (r.choices[0].message.content or \"\").strip().splitlines()[0].strip()\n",
        "        if choice.upper() == \"NONE\":\n",
        "            return None\n",
        "        return choice if choice in candidates else None\n",
        "    except Exception:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPF7cMTTAm4E"
      },
      "source": [
        "#Verificar chunks\n",
        "\n",
        "Muestra el primer chunk de cada archivo.\n",
        "\n",
        "Sirve como herramienta de debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "04TJfz_sQRkN"
      },
      "outputs": [],
      "source": [
        "# Verificaci√≥n de chunks (primer chunk por CV)\n",
        "def verificar_chunks():\n",
        "    import os\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "    textos_por_cv = []\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "    for fname in os.listdir(DOCS_DIR):\n",
        "        filepath = os.path.join(DOCS_DIR, fname)\n",
        "        if not os.path.isfile(filepath):\n",
        "            continue\n",
        "        if not (fname.endswith(\".txt\") or fname.endswith(\".pdf\")):\n",
        "            continue\n",
        "\n",
        "        # Leer archivo con robustez\n",
        "        try:\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(filepath, \"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "        # Limpiar texto\n",
        "        text = text.replace(\"\\x00\", \"\").strip()\n",
        "\n",
        "        # Dividir en chunks\n",
        "        chunks = splitter.split_text(text)\n",
        "        primer_chunk = chunks[0] if chunks else \"(vac√≠o o ilegible)\"\n",
        "        textos_por_cv.append(f\"**{fname}**\\n{primer_chunk}\")\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(textos_por_cv) or \" No se encontraron archivos v√°lidos.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "7hPfk6e_Kcd5",
        "outputId": "0e69e92b-94d0-43ec-e42a-f3ca39657b71"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**cv1.txt**\\n# CV Corporativo - Ricardo Lopez\\n\\n## Datos Personales\\n- Nombre: Ricardo Lopez  \\n- Edad: 47 anios  \\n- Correo: ricardo.lopez.corporativo@example.com  \\n- Ubicacion: Buenos Aires, Argentina  \\n- LinkedIn: linkedin.com/in/ricardolopez-corporativo\\n\\n---\\n\\n**cv2.txt**\\n# CV Tecnologico - Sofia Martinez\\n\\n## Datos Personales\\n- Nombre: Sofia Martinez  \\n- Edad: 38 anios  \\n- Correo: sofia.martinez.tecnologico@example.com  \\n- Ubicacion: Mendoza, Argentina  \\n- LinkedIn: linkedin.com/in/sofiamartinez-tecnologico  \\n\\n## Resumen Profesional\\n- Especialista en desarrollo de software, arquitecturas de datos e inteligencia artificial.  \\n- Experiencia en startups y empresas multinacionales.  \\n- Enfoque en soluciones escalables, seguras y eficientes en la nube.'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "verificar_chunks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xywxEijABqF"
      },
      "source": [
        "#Chat multi-agente (RAG)\n",
        "\n",
        "Usa el router para elegir √≠ndice.\n",
        "\n",
        "Recupera contexto con vectores.\n",
        "\n",
        "Llama al modelo LLM con contexto, pregunta e historial.\n",
        "\n",
        "Devuelve respuesta + fuentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OXaCLf6Xjbt",
        "outputId": "4edd157c-6859-4dc3-d453-3892274f298b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_cv_indices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "s0MYWw25AUV9"
      },
      "outputs": [],
      "source": [
        "def chat(user_message, chat_history):\n",
        "    \"\"\"\n",
        "    - Router elige el √≠ndice (CV) m√°s adecuado.\n",
        "    - Retrieve desde ese √≠ndice (usando PARAMS['k']).\n",
        "    - Groq responde con el contexto recuperado (usando temp/top_p/max_tokens de PARAMS).\n",
        "    - Devuelve UN string (respuesta + fuentes).\n",
        "    \"\"\"\n",
        "    # Lee par√°metros actuales elegidos por el usuario\n",
        "    K = int(PARAMS[\"k\"])\n",
        "    TEMPERATURE = float(PARAMS[\"temperature\"])\n",
        "    TOP_P = float(PARAMS[\"top_p\"])\n",
        "    MAX_TOKENS = int(PARAMS[\"max_tokens\"])\n",
        "\n",
        "    # 1) Router: elegir √≠ndice de CV\n",
        "    candidates = list_cv_indices()\n",
        "    chosen_index = router_agent(user_message, candidates)\n",
        "\n",
        "    # 2) Retrieve desde el √≠ndice elegido (o mensaje si no hay)\n",
        "    if chosen_index:\n",
        "        matches = retrieve_from_index(chosen_index, user_message, top_k=K)\n",
        "        chosen_info = chosen_index\n",
        "    else:\n",
        "        matches = []\n",
        "        chosen_info = \"‚Äî (sin selecci√≥n)\"\n",
        "\n",
        "    context_text = build_context(matches)\n",
        "\n",
        "    # 3) Armar mensajes para Groq\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    messages.append({\"role\": \"system\", \"content\": f\"√çndice seleccionado por el router: {chosen_info}\"})\n",
        "    messages.append({\"role\": \"system\", \"content\": f\"Contexto recuperado:\\n\\n{context_text}\"})\n",
        "\n",
        "    for u, b in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": u})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": b})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # 4) Llamada Groq con par√°metros elegidos\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",\n",
        "        messages=messages,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        max_tokens=MAX_TOKENS\n",
        "    )\n",
        "    bot_reply = resp.choices[0].message.content\n",
        "\n",
        "    # 5) A√±adir pie de fuentes en el MISMO string\n",
        "    if matches:\n",
        "        fuentes = \"\\n\".join([f\"‚Ä¢ {m['archivo']} (score={m['score']})\" for m in matches])\n",
        "    else:\n",
        "        fuentes = \"‚Äî\"\n",
        "    bot_reply_with_sources = (\n",
        "        f\"{bot_reply}\\n\\n---\\n\"\n",
        "        f\"**Router ‚Üí √≠ndice seleccionado:** {chosen_info}\\n\"\n",
        "        f\"**Fuentes (pasajes recuperados):**\\n{fuentes}\"\n",
        "    )\n",
        "\n",
        "    # 6) Guardar historial (nuestra simulaci√≥n de contexto)\n",
        "    history.append((user_message, bot_reply_with_sources))\n",
        "\n",
        "    return bot_reply_with_sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqalmkhiC5oY"
      },
      "source": [
        "#Memoria del Chat\n",
        "\n",
        "Y se actualiza en la √∫ltima l√≠nea de la funci√≥n chat():\n",
        "\n",
        "La memoria se acumula en esta variable global:\n",
        "\n",
        "Cada vez que el usuario env√≠a un mensaje, se guarda el par (pregunta, respuesta) en history.\n",
        "\n",
        "Esta history se reutiliza en cada nuevo mensaje para simular ‚Äúmemoria del chat‚Äù.\n",
        "\n",
        "La historia se convierte en parte del prompt enviado al modelo LLM (Groq):\n",
        "\n",
        "for u, b in history:\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": u})\n",
        "    messages.append({\"role\": \"assistant\", \"content\": b})\n",
        "\n",
        "Esto permite que el modelo entienda el contexto conversacional anterior.\n",
        "\n",
        "Memoria del Chat\n",
        "\n",
        "El modelo que est√°s usando es: \"llama3-8b-8192\"\n",
        "\n",
        "Esto significa que su ventana de contexto es de 8192 tokens (m√°ximo que puede procesar en cada mensaje).\n",
        "\n",
        "Todo lo siguiente consume tokens:\n",
        "\n",
        "System prompts (prompts iniciales)\n",
        "\n",
        "Texto recuperado por RAG (context_text)\n",
        "\n",
        "Historial (history)\n",
        "\n",
        "Nueva pregunta del usuario\n",
        "\n",
        "Si todo eso suma m√°s de 8192 tokens, el modelo recortar√° autom√°ticamente el prompt, y puede ignorar partes viejas del historial o contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pUFFzQ3SDCVA"
      },
      "outputs": [],
      "source": [
        "history = []  # [(user, bot), ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QyLTX3JOWh29"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any, Optional, Tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QoPl0_IGWSx9"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Versi√≥n extendida: Multiagente y agente √∫nico\n",
        "# =========================\n",
        "\n",
        "def router_agent_single(user_question: str, candidates: List[str]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Elige UN √∫nico √≠ndice de CV seg√∫n la pregunta del usuario.\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return None\n",
        "    cand_list = \"\\n\".join(f\"- {c}\" for c in candidates)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": ROUTER_SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": f\"\\u00cdndices disponibles:\\n{cand_list}\\n\\nPregunta: {user_question}\\n\\nDevuelve SOLO un nombre EXACTO de la lista, o 'NONE' si ninguno aplica.\"}\n",
        "    ]\n",
        "    try:\n",
        "        r = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=messages,\n",
        "            temperature=0.0,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        choice = (r.choices[0].message.content or \"\").strip().splitlines()[0].strip()\n",
        "        return choice if choice.upper() != \"NONE\" and choice in candidates else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def router_agent_multi(user_question: str, candidates: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Devuelve una lista de √≠ndices relevantes seg√∫n la pregunta del usuario.\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "    cand_list = \"\\n\".join(f\"- {c}\" for c in candidates)\n",
        "    system_prompt = (\n",
        "        \"Eres un enrutador inteligente. Tu tarea es elegir todos los √≠ndices de la lista que sean relevantes para la consulta del usuario.\\n\"\n",
        "        \"Responde con una lista separada por comas, usando exactamente los nombres tal como aparecen.\\n\"\n",
        "        \"Si ninguno aplica, responde 'NONE'.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"\\u00cdndices disponibles:\\n{cand_list}\\n\\nPregunta: {user_question}\\n\\nDevuelve una lista separada por comas o 'NONE'.\"}\n",
        "    ]\n",
        "    try:\n",
        "        r = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=messages,\n",
        "            temperature=0.0,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        raw = (r.choices[0].message.content or \"\").strip()\n",
        "        if raw.upper() == \"NONE\":\n",
        "            return []\n",
        "        selected = [s.strip() for s in raw.split(\",\") if s.strip() in candidates]\n",
        "        return selected\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def chat_multiagent(user_message: str, chat_history: List[Tuple[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    - Usa el router multiagente para detectar todos los CVs relevantes.\n",
        "    - Consulta a cada √≠ndice por separado y concatena el contexto.\n",
        "    - Genera una √∫nica respuesta compuesta con fuentes diferenciadas.\n",
        "    \"\"\"\n",
        "    K = int(PARAMS[\"k\"])\n",
        "    TEMPERATURE = float(PARAMS[\"temperature\"])\n",
        "    TOP_P = float(PARAMS[\"top_p\"])\n",
        "    MAX_TOKENS = int(PARAMS[\"max_tokens\"])\n",
        "\n",
        "    candidates = list_cv_indices()\n",
        "    selected_indices = router_agent_multi(user_message, candidates)\n",
        "\n",
        "    all_matches = []\n",
        "    all_contexts = []\n",
        "    index_labels = []\n",
        "\n",
        "    for idx in selected_indices:\n",
        "        matches = retrieve_from_index(idx, user_message, top_k=K)\n",
        "        context = build_context(matches)\n",
        "        all_matches.extend(matches)\n",
        "        index_labels.append(idx)\n",
        "        all_contexts.append(f\"## Contexto de {idx}\\n\\n{context}\")\n",
        "\n",
        "    combined_context = \"\\n\\n\".join(all_contexts) if all_contexts else \"(no se recuper√≥ contexto)\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"system\", \"content\": f\"√çndices seleccionados: {', '.join(index_labels) if index_labels else '‚Äî (ninguno)'}\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Contexto recuperado:\\n\\n{combined_context}\"}\n",
        "    ]\n",
        "\n",
        "    for u, b in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": u})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": b})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",\n",
        "        messages=messages,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        max_tokens=MAX_TOKENS\n",
        "    )\n",
        "    bot_reply = resp.choices[0].message.content\n",
        "\n",
        "    if all_matches:\n",
        "        fuentes = \"\\n\".join([f\"‚Ä¢ {m['archivo']} (score={m['score']})\" for m in all_matches])\n",
        "    else:\n",
        "        fuentes = \"‚Äî\"\n",
        "\n",
        "    bot_reply_with_sources = (\n",
        "        f\"{bot_reply}\\n\\n---\\n\"\n",
        "        f\"**Router ‚Üí √≠ndices seleccionados:** {', '.join(index_labels) if index_labels else '‚Äî'}\\n\"\n",
        "        f\"**Fuentes:**\\n{fuentes}\"\n",
        "    )\n",
        "\n",
        "    chat_history.append((user_message, bot_reply_with_sources))\n",
        "    return bot_reply_with_sources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "A8F6ywfHW1vY"
      },
      "outputs": [],
      "source": [
        "PARAMS = {\n",
        "    \"k\": 10,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.7,\n",
        "    \"max_tokens\": 512\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iOZh2JGYSX2",
        "outputId": "d7b9fa3e-2828-4406-a59c-a4bf9f4386fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexado OK. Archivos: 2 | √çndices nuevos: 2 | Actualizados: 0 | Chunks insertados: 16\n"
          ]
        }
      ],
      "source": [
        "folder = \"/content/cv\"  # o donde tengas tus .pdf/.txt\n",
        "msg = upsert_documents_per_cv(folder)\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67uTPQtGXQug",
        "outputId": "b4640a80-172f-4533-9109-536264a2e1b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cv--cv1', 'cv--cv2']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_cv_indices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "F0a_cGZ3WSu_",
        "outputId": "c0da9a59-be01-4292-d1a7-72b7c7407d9c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Contexto de cv--cv2\\n\\nSofia Martinez:\\n\\n* Especialista en desarrollo de software, arquitecturas de datos e inteligencia artificial\\n* Experiencia en sistemas distribuidos, automatizaci√≥n de procesos empresariales mediante RPA y despliegue de modelos de IA con MLOps en entornos cloud (AWS, GCP)\\n* Diplomatura en Deep Learning - MIT\\n* Experiencia en startups y empresas multinacionales\\n* Enfoque en soluciones escalables, seguras y eficientes en la nube\\n\\n---\\n**Router ‚Üí √≠ndices seleccionados:** cv--cv1, cv--cv2\\n**Fuentes:**\\n‚Ä¢ cv1.txt (score=0.3898)\\n‚Ä¢ cv1.txt (score=0.3887)\\n‚Ä¢ cv1.txt (score=0.2385)\\n‚Ä¢ cv1.txt (score=0.2352)\\n‚Ä¢ cv2.txt (score=0.5111)\\n‚Ä¢ cv2.txt (score=0.4835)\\n‚Ä¢ cv2.txt (score=0.2532)\\n‚Ä¢ cv2.txt (score=0.2049)'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_multiagent(\"resume el cv de sofia\", [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "1TvXwWyOWSpg",
        "outputId": "b5e9f300-53bd-4df8-b7a7-19ecce732ba6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Seg√∫n el contexto, el CV de Ricardo Lopez es el siguiente:\\n\\n* Es un ejecutivo con experiencia en liderazgo empresarial y gesti√≥n de proyectos tecnol√≥gicos.\\n* Especialista en transformaci√≥n digital y gesti√≥n de equipos multidisciplinarios.\\n* Enfoque en cumplimiento de KPIs y desarrollo de cultura basada en datos.\\n* Experiencia laboral como Director de Transformaci√≥n Digital en TechCorp y Gerente de Proyectos en Empresa XYZ.\\n* Implement√≥ estrategias de innovaci√≥n y lider√≥ equipos en telecomunicaciones y otros sectores.\\n* Ubicado en Buenos Aires, Argentina y tiene un perfil en LinkedIn.\\n\\n---\\n**Router ‚Üí √≠ndices seleccionados:** cv--cv1, cv--cv2\\n**Fuentes:**\\n‚Ä¢ cv1.txt (score=0.5742)\\n‚Ä¢ cv1.txt (score=0.4811)\\n‚Ä¢ cv1.txt (score=0.2956)\\n‚Ä¢ cv1.txt (score=0.2463)\\n‚Ä¢ cv2.txt (score=0.4253)\\n‚Ä¢ cv2.txt (score=0.3512)\\n‚Ä¢ cv2.txt (score=0.2441)\\n‚Ä¢ cv2.txt (score=0.173)'"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_multiagent(\"resume el cv de ricardo\", [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "fGqU2np3WSfN",
        "outputId": "1e686c3a-bba0-471b-fef4-de3facc4ca9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Seg√∫n el contexto, Ricardo Lopez (cv--cv1) tiene 47 a√±os, mientras que Sofia Martinez (cv--cv2) tiene 38 a√±os. Por lo tanto, Ricardo Lopez es el candidato m√°s grande en edad.\\n\\n---\\n**Router ‚Üí √≠ndices seleccionados:** cv--cv1, cv--cv2\\n**Fuentes:**\\n‚Ä¢ cv1.txt (score=0.3716)\\n‚Ä¢ cv1.txt (score=0.3489)\\n‚Ä¢ cv1.txt (score=0.3201)\\n‚Ä¢ cv1.txt (score=0.3084)\\n‚Ä¢ cv2.txt (score=0.3146)\\n‚Ä¢ cv2.txt (score=0.2824)\\n‚Ä¢ cv2.txt (score=0.2813)\\n‚Ä¢ cv2.txt (score=0.2049)'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_multiagent(\"Cual de los dos candidatos es mas grande en edad\", [])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rG-Bn4BZtV0"
      },
      "source": [
        "#Interfaz de usuario (Gradio)\n",
        "\n",
        "Incluye:\n",
        "\n",
        "Bot√≥n para reindexar.\n",
        "\n",
        "Sliders para k, temp, top_p, max_tokens.\n",
        "\n",
        "Chat interface.\n",
        "\n",
        "Herramienta para verificar chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "2N4nMCVBcG8G",
        "outputId": "bfd7bfb9-f969-4bc3-d72c-8f32b1e5106f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-150938962.py:80: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbox = gr.Chatbot()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://54eb75b60747bd00d9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://54eb75b60747bd00d9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =========================\n",
        "# Interfaz Gradio final ‚Äî Multiagente con ejemplos y limpieza\n",
        "# =========================\n",
        "\n",
        "PARAMS = {\n",
        "    \"k\": TOP_K_DEFAULT,\n",
        "    \"temperature\": TEMPERATURE_DEFAULT,\n",
        "    \"top_p\": TOP_P_DEFAULT,\n",
        "    \"max_tokens\": MAX_TOKENS_DEFAULT,\n",
        "}\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "# Reindexar documentos\n",
        "\n",
        "def reindex_action():\n",
        "    os.makedirs(DOCS_DIR, exist_ok=True)\n",
        "    return upsert_documents_per_cv(DOCS_DIR)\n",
        "\n",
        "# Borrar todos los √≠ndices (para limpieza completa)\n",
        "def reset_all():\n",
        "    deleted = []\n",
        "    for idx in list_cv_indices():\n",
        "        try:\n",
        "            pc.delete_index(idx)\n",
        "            deleted.append(idx)\n",
        "        except Exception:\n",
        "            pass\n",
        "    chat_history.clear()\n",
        "    return f\"Se eliminaron los √≠ndices: {', '.join(deleted)}\\nHistorial reiniciado.\"\n",
        "\n",
        "# Borrar solo el historial\n",
        "\n",
        "def limpiar_chat():\n",
        "    chat_history.clear()\n",
        "    return [], \"\"\n",
        "\n",
        "# Callbacks sliders\n",
        "\n",
        "def _set_k(v):           PARAMS[\"k\"] = int(v)\n",
        "def _set_temp(v):        PARAMS[\"temperature\"] = float(v)\n",
        "def _set_top_p(v):       PARAMS[\"top_p\"] = float(v)\n",
        "def _set_max_tokens(v):  PARAMS[\"max_tokens\"] = int(v)\n",
        "\n",
        "# Interfaz\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## RAG multi-agente por CV ‚Äî Groq + Pinecone\")\n",
        "    gr.Markdown(\"Sub√≠ tus CVs en `/content/cv` (.pdf o .txt) y presion√° **Reindexar** para crear un √≠ndice por cada CV.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        re_btn = gr.Button(\"Reindexar (/content/cv)\")\n",
        "        reset_btn = gr.Button(\"üßπ Limpiar todo (historial + √≠ndices)\")\n",
        "    re_out = gr.Textbox(label=\"Estado\", lines=3)\n",
        "    re_btn.click(fn=reindex_action, outputs=[re_out])\n",
        "    reset_btn.click(fn=reset_all, outputs=[re_out])\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### Par√°metros de recuperaci√≥n y generaci√≥n\")\n",
        "\n",
        "    with gr.Row():\n",
        "        k_slider = gr.Slider(label=\"Top-K retrieve\", minimum=1, maximum=10, step=1, value=PARAMS[\"k\"])\n",
        "        temp_slider = gr.Slider(label=\"Temperature\", minimum=0.0, maximum=1.5, step=0.1, value=PARAMS[\"temperature\"])\n",
        "        top_p_slider = gr.Slider(label=\"Top-p\", minimum=0.1, maximum=1.0, step=0.05, value=PARAMS[\"top_p\"])\n",
        "        max_tk_slider = gr.Slider(label=\"Max tokens\", minimum=64, maximum=2048, step=32, value=PARAMS[\"max_tokens\"])\n",
        "\n",
        "    k_slider.change(_set_k, inputs=[k_slider])\n",
        "    temp_slider.change(_set_temp, inputs=[temp_slider])\n",
        "    top_p_slider.change(_set_top_p, inputs=[top_p_slider])\n",
        "    max_tk_slider.change(_set_max_tokens, inputs=[max_tk_slider])\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### Modo de operaci√≥n\")\n",
        "    modo_multi = gr.Checkbox(label=\"Usar modo multiagente (varios CVs) üí°\", value=True)\n",
        "\n",
        "    gr.Markdown(\"### Chat con el sistema\")\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(label=\"Tu pregunta\", lines=2, scale=8)\n",
        "        send_btn = gr.Button(\"Enviar\", scale=2)\n",
        "\n",
        "    chatbox = gr.Chatbot()\n",
        "    clear_btn = gr.Button(\"üßº Limpiar chat\")\n",
        "\n",
        "    # Ejemplos\n",
        "    ejemplos = [\n",
        "        \"¬øEn qu√© proyectos de ciencia de datos particip√≥ Ricardo?\",\n",
        "        \"¬øQu√© experiencia tienen Sof√≠a y Ricardo con sklearn, XGBoost o LightGBM?\",\n",
        "        \"¬øCu√°l de los candidatos tiene un perfil m√°s acad√©mico?\",\n",
        "        \"¬øCu√°l de los candidatos tiene un perfil m√°s corporativo?\",\n",
        "        \"¬øQui√©n de los candidatos sabe Python y trabaj√≥ en miner√≠a?\",\n",
        "        \"¬øQu√© personas tienen experiencia en proyectos internacionales y manejo de AWS o GCP?\",\n",
        "        \"Estoy buscando alguien con conocimientos en ML, datos y experiencia minera. ¬øA qui√©n considero?\",\n",
        "        \"¬øQui√©nes lideraron equipos t√©cnicos y usaron Power BI o Tableau?\"\n",
        "    ]\n",
        "    gr.Examples(examples=ejemplos, inputs=msg)\n",
        "\n",
        "    # L√≥gica de respuesta\n",
        "    def responder(input_text, modo_multi):\n",
        "          if not input_text.strip():\n",
        "              return []\n",
        "\n",
        "          print(\"üîß Par√°metros actuales:\")\n",
        "          print(PARAMS)\n",
        "\n",
        "          if modo_multi:\n",
        "              respuesta = chat_multiagent(input_text, chat_history)\n",
        "          else:\n",
        "              respuesta = chat(input_text, chat_history)\n",
        "\n",
        "          return [(input_text, respuesta)]\n",
        "\n",
        "    send_btn.click(fn=responder, inputs=[msg, modo_multi], outputs=[chatbox])\n",
        "    msg.submit(fn=responder, inputs=[msg, modo_multi], outputs=[chatbox]).then(\n",
        "        fn=lambda: \"\", inputs=None, outputs=msg\n",
        "    )\n",
        "\n",
        "    clear_btn.click(fn=limpiar_chat, outputs=[chatbox, msg])\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### Vista de chunks iniciales\")\n",
        "    verif_btn = gr.Button(\"Verificar primer chunk de cada CV\")\n",
        "    verif_out = gr.Textbox(label=\"Preview de chunks\", lines=10)\n",
        "    verif_btn.click(fn=verificar_chunks, outputs=[verif_out])\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4CLI7O2Zp_b"
      },
      "source": [
        "#DEBUGGING VERIFICACION MANUAL\n",
        "\n",
        "Es para ejecutar paso a paso todo el flujo con un archivo espec√≠fico, esto para analizar los errores que fueron surgiendo:\n",
        "\n",
        "Lee archivo.\n",
        "\n",
        "Divide en chunks.\n",
        "\n",
        "Calcula embeddings.\n",
        "\n",
        "Hace upsert en Pinecone.\n",
        "\n",
        "Elimina √≠ndices si es necesario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x0pMW5-2GgB",
        "outputId": "966b7cc6-568d-4999-cbdf-6bdbbae51fa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto le√≠do:\n",
            "('# CV Corporativo - Ricardo Lopez ## Datos Personales - Nombre: Ricardo Lopez '\n",
            " '- Edad: 47 anios - Correo: ricardo.lopez.corporativo@example.com - '\n",
            " 'Ubicacion: Buenos Aires, Argentina - LinkedIn: '\n",
            " 'linkedin.com/in/ricardolopez-corporativo ## Resumen Profesional - Ejecutivo '\n",
            " 'con experiencia en liderazgo empresarial y gestion de proyectos '\n",
            " 'tecnologicos. - Especialista en transformacion digital y gestion de equipos '\n",
            " 'multidisciplinarios. - Enfoque en cumplimiento de KPIs y desarrollo de '\n",
            " 'cultura basada en datos. ## Experiencia Laboral - **Director de '\n",
            " 'Transformacion Digital en TechCorp**: implementacion de estrategias de '\n",
            " 'innovacion. - **Gerente de Proyectos en Empresa XYZ**: liderazgo de equipos '\n",
            " 'en telecomunicaciones y finanzas. - **Consultor Senior en Deloitte**: '\n",
            " 'asesoria en analitica de datos y gestion de riesgos. ## Logros Destacados - '\n",
            " 'Reduccion de costos operativos en un 20% mediante procesos de '\n",
            " 'digitalizacion. - Implementacion de tableros de control para seguimiento de '\n",
            " 'indicadores clave. - Desarr')\n",
            "\n",
            "Chunks generados: 8\n",
            "--- Chunk 1 ---\n",
            "# CV Corporativo - Ricardo Lopez ## Datos Personales - Nombre: Ricardo Lopez - Edad: 47 anios - Correo: ricardo.lopez.corporativo@example.com - Ubicacion: Buenos Aires, Argentina - LinkedIn:\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Argentina - LinkedIn: linkedin.com/in/ricardolopez-corporativo ## Resumen Profesional - Ejecutivo con experiencia en liderazgo empresarial y gestion de proyectos tecnologicos. - Especialista en\n",
            "\n",
            "--- Chunk 3 ---\n",
            "- Especialista en transformacion digital y gestion de equipos multidisciplinarios. - Enfoque en cumplimiento de KPIs y desarrollo de cultura basada en datos. ## Experiencia Laboral - **Director de\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "archivo = \"/content/cv/cv1.txt\"\n",
        "text = read_txt(archivo)\n",
        "print(\"Texto le√≠do:\")\n",
        "pprint(text[:1000])\n",
        "\n",
        "chunks = chunk_text(text)\n",
        "print(f\"\\nChunks generados: {len(chunks)}\")\n",
        "for i, c in enumerate(chunks[:3]):\n",
        "    print(f\"--- Chunk {i+1} ---\\n{c}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ck2erb2pOj",
        "outputId": "d046470d-222f-4395-db9f-1664ca099d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/cv/!file:   cannot open `/content/cv/!file' (No such file or directory)\n",
            "/content/cv/cv1.txt: text/plain; charset=us-ascii\n",
            "/content/cv/cv2.txt: text/plain; charset=us-ascii\n"
          ]
        }
      ],
      "source": [
        "!file -i /content/cv/!file -i /content/cv/*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIHkQlyb4zMO",
        "outputId": "28967ffc-fc35-44ea-a464-3f4882c86bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# CV Corporativo - Ricardo Lopez\n",
            "\n",
            "## Datos Personales\n",
            "- Nombre: Ricardo Lopez  \n",
            "- Edad: 47 anios  \n",
            "- Correo: ricardo.lopez.corporativo@example.com  \n",
            "- Ubicacion: Buenos Aires, Argentina  \n",
            "- LinkedIn: linkedin.com/in/ricardolopez-corporativo  \n",
            "\n",
            "## Resumen Profesional\n",
            "- Ejecutivo con experiencia en liderazgo empresarial y gestion de proyectos tecnologicos.  \n",
            "- Especialista en transformacion digital y gestion de equipos multidisciplinarios.  \n",
            "- Enfoque en cumplimiento de KPIs y desarrollo de cultur\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/cv/cv1.txt\", encoding=\"latin-1\") as f:\n",
        "    print(f.read()[:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hJTXBeB49jx",
        "outputId": "51ed701d-c816-4fdf-d41c-de325d7aed80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8 vectores de embeddings generados.\n",
            "[-0.02830718830227852, -0.03697291761636734, -0.043179091066122055, 0.014025023207068443, 0.09206068515777588]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    vecs = embed_texts(chunks)\n",
        "    print(f\"{len(vecs)} vectores de embeddings generados.\")\n",
        "    print(vecs[0][:5])  # Mostrar los primeros 5 valores del primer vector\n",
        "except Exception as e:\n",
        "    print(f\"Error al generar embeddings: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We8ncrJ96Ga9",
        "outputId": "c440509f-a19b-4a2d-8a2c-995791ec7981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upserting batch 0‚Äì200 en √≠ndice 'cv--cv1'\n",
            "Error al hacer upsert en Pinecone: 'NoneType' object is not callable\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    BATCH = 200\n",
        "    for i in range(0, len(vectors), BATCH):\n",
        "        print(f\"Upserting batch {i}‚Äì{i+BATCH} en √≠ndice '{index_name}'\")\n",
        "        idx.upsert(vectors=vectors[i:i+BATCH])\n",
        "except Exception as e:\n",
        "    print(f\"Error al hacer upsert en Pinecone: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "idHsmYuL7021"
      },
      "outputs": [],
      "source": [
        "indices = pc.list_indexes()\n",
        "for idx in indices:\n",
        "    nombre = idx[\"name\"]\n",
        "    if nombre.startswith(\"cv-\"):\n",
        "        pc.delete_index(nombre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OwRFHw76gSp",
        "outputId": "37e4e46b-6ae6-40bc-91d1-6f568f3a4065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Procesando archivo: cv1.txt\n",
            "√çndice: cv--cv1\n",
            "8 chunks generados\n",
            "8 vectores generados\n",
            "Subiendo batch 0‚Äì200\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "path = Path(\"/content/cv/cv1.txt\")\n",
        "\n",
        "base_noext = path.stem\n",
        "index_name = sanitize_index_name(base_noext)\n",
        "\n",
        "print(f\"\\n Procesando archivo: {path.name}\")\n",
        "print(f\"√çndice: {index_name}\")\n",
        "\n",
        "ensure_index(index_name)\n",
        "\n",
        "raw_text = read_file(path).strip()\n",
        "if not raw_text:\n",
        "    print(f\"Vac√≠o o ilegible: {path.name}\")\n",
        "else:\n",
        "    chunks = chunk_text(raw_text)\n",
        "    if not chunks:\n",
        "        print(f\"Sin chunks: {path.name}\")\n",
        "    else:\n",
        "        print(f\"{len(chunks)} chunks generados\")\n",
        "        try:\n",
        "            vecs = embed_texts(chunks)\n",
        "            print(f\"{len(vecs)} vectores generados\")\n",
        "            vectors = []\n",
        "            for i, vec in enumerate(vecs):\n",
        "                vid = f\"{base_noext}-{i:04d}\"\n",
        "                vectors.append({\n",
        "                    \"id\": vid,\n",
        "                    \"values\": vec,\n",
        "                    \"metadata\": {\n",
        "                        \"texto\": chunks[i],\n",
        "                        \"archivo\": path.name,\n",
        "                        \"chunk_id\": i,\n",
        "                        \"fecha\": datetime.today().strftime(\"%Y-%m-%d\")\n",
        "                    }\n",
        "                })\n",
        "\n",
        "            idx = pc.Index(index_name)\n",
        "            for i in range(0, len(vectors), 200):\n",
        "                print(f\"Subiendo batch {i}‚Äì{i+200}\")\n",
        "                idx.upsert(vectors=vectors[i:i+200])\n",
        "        except Exception as e:\n",
        "            print(f\"Error al generar embeddings o subir: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
